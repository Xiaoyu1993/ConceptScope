<!DOCTYPE html>
<!-- saved from url=(0227)http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&id=3300271&acc=OA&key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&__acm__=1575108226_2c198cd880d01f330d94a51502861377 -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <title>Toward Algorithmic Accountability in Public Services</title> <!-- Copyright (c) 2010-2015 The MathJax Consortium --> <meta name="viewport" content="width=device-width; initial-scale=1.0;"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <link media="screen, print" rel="stylesheet" href="./paper-chi-algorithm-4_files/bootstrap.min.css"><link media="screen, print" rel="stylesheet" href="./paper-chi-algorithm-4_files/bootstrap-theme.min.css"><link media="screen, print" rel="stylesheet" href="./paper-chi-algorithm-4_files/main.css"><script src="./paper-chi-algorithm-4_files/jquery.min.js" type="text/javascript"></script> <script src="./paper-chi-algorithm-4_files/bootstrap.min.js" type="text/javascript"></script> <script src="./paper-chi-algorithm-4_files/bibCit.js" type="text/javascript"></script> <script src="./paper-chi-algorithm-4_files/divTab.js" type="text/javascript"></script> <script type="text/javascript" src="./paper-chi-algorithm-4_files/MathJax.js"></script> <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head> <body id="main"><div id="MathJax_Message" style="display: none;"></div><table width="100%"><tbody><tr><td><div class="utilities-area"><div class="logo-section"><div class="show-for-large-up"><a class="navbar-brand" href="http://www.acm.org/"> <img alt="ACM Logo" class="img-responsive" src="./paper-chi-algorithm-4_files/acm_logo.jpg"></a></div><div class="hide-for-large-up"><a class="navbar-brand" href="http://www.acm.org/"> <img alt="ACM Logo" class="img-responsive" src="./paper-chi-algorithm-4_files/acm_logo_mobile.jpg"></a></div></div></div></td></tr><tr height="50px"><td align="left"><button class="ArtNav" id="ArtNav" onclick="openNav(this)" tabindex="0"><span aria-hidden="true">☰</span><span class="ArticleNavi"> Article Navigation</span></button></td></tr></tbody></table> <div id="mySidenav" class="sidenav" aria-hidden="true" tabindex="-1" role="region" aria-labelledby="sidebar_title"><span id="sidebar_title" class="navHead" align="center">Article Navigation</span><a href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#" aria-label="Close article navigation" class="closebtn" onclick="closeNav(true)" tabindex="-1"><span aria-hidden="true">×</span></a><a href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#" class="navAbs" onclick="closeNav(false)" tabindex="-1">Abstract</a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-2"><span style="vertical-align: top;">1</span><span style="display:inline-block;margin-left:5px;width:80%">  INTRODUCTION</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-3"><span style="vertical-align: top;">2</span><span style="display:inline-block;margin-left:5px;width:80%">  BACKGROUND AND MOTIVATION</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-4"><span style="vertical-align: top;">3</span><span style="display:inline-block;margin-left:5px;width:80%">  METHODOLOGY</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-5"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Participants</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-6"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Workshops</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-7"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Data collection and analysis</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-8"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Study limitations</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-9"><span style="vertical-align: top;">4</span><span style="display:inline-block;margin-left:5px;width:80%">  FINDINGS</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-10"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  System-level concerns were the most common reasons given for low comfort in algorithm-assisted and algorithmic decision-making</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-11"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  All groups raised concerns about potential bias on the part of case workers involved in the decision process, as well as bias present in the data or the algorithm</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-12"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Participants questioned whether a statistical model could adequately account for all relevant decision elements, and emphasized the need for a human in the loop approach</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-13"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Participants wanted more information on how the algorithm weighs different factors, and the ability to dispute the score</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-14"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Even potentially beneficial decisions resulted in discomfort due to concerns about how and whether risk information was communicated to families and case workers</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-15"><span style="vertical-align: top;">5</span><span style="display:inline-block;margin-left:5px;width:80%">  DISCUSSION</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-16"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Address system-level concerns.</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-17"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Weigh positive and negative data in algorithmic decision-making.</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-18"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Convey how making use of data and algorithms leads to improved family and process outcomes.</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-19"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Facilitate supportive communication and positive relationships between child welfare workers and families.</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-20"><span style="vertical-align: top;">6</span><span style="display:inline-block;margin-left:5px;width:80%">  CONCLUSION</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-21"><span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  ACKNOWLEDGMENTS</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#ref-001"><span style="vertical-align: top;"></span><span style="display:inline-block;margin-left:5px;width:80%">REFERENCES</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#foot-001"><span style="vertical-align: top;"></span><span style="display:inline-block;margin-left:5px;width:80%">FOOTNOTE</span></a></div><section class="front-matter"> <section> <header class="title-info"> <div class="journal-title"> <h1> <span class="title">Toward Algorithmic Accountability in Public Services</span> <br> <span class="subTitle">A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-Making in Child Welfare Services</span> </h1> </div> </header> <div class="authorGroup"> <div class="author"> <span class="givenName">Anna</span> <span class="surName">Brown</span>, Massey University, New Zealand, <a href="mailto:a.e.brown@massey.ac.nz">a.e.brown@massey.ac.nz</a> </div> <div class="author"> <span class="givenName">Alexandra</span> <span class="surName">Chouldechova</span>, Carnegie Mellon University, USA, <a href="mailto:achould@cmu.edu">achould@cmu.edu</a> </div> <div class="author"> <span class="givenName">Emily</span> <span class="surName">Putnam-Hornstein</span>, University of Southern California, USA, <a href="mailto:hornste@usc.edu">hornste@usc.edu</a> </div> <div class="author"> <span class="givenName">Andrew</span> <span class="surName">Tobin</span>, Massey University, New Zealand, <a href="mailto:a.tobin@massey.ac.nz">a.tobin@massey.ac.nz</a> </div> <div class="author"> <span class="givenName">Rhema</span> <span class="surName">Vaithianathan</span>, Auckland University of Technology, New Zealand, <a href="mailto:rhema.vaithianathan@aut.ac.nz">rhema.vaithianathan@aut.ac.nz</a> </div> </div> <br> <div class="pubInfo"> <p>DOI: <a href="https://doi.org/10.1145/3290605.3300271" target="_blank">https://doi.org/10.1145/3290605.3300271</a> <br>CHI '19: <a href="https://doi.org/10.1145/3290605" target="_blank">Proceedings of CHI Conference on Human Factors in Computing Systems</a>, Glasgow, Scotland UK, May 2018</p> </div> <div class="abstract"> <p><small>Algorithmic decision-making systems are increasingly being adopted by government public service agencies. Researchers, policy experts, and civil rights groups have all voiced concerns that such systems are being deployed without adequate consideration of potential harms, disparate impacts, and public accountability practices. Yet little is known about the concerns of those most likely to be affected by these systems. We report on workshops conducted to learn about the concerns of affected communities in the context of child welfare services. The workshops involved 83 study participants including families involved in the child welfare system, employees of child welfare agencies, and service providers. Our findings indicate that general distrust in the existing system contributes significantly to low comfort in algorithmic decision-making. We identify strategies for improving comfort through greater transparency and improved communication strategies. We discuss the implications of our study for accountable algorithm design for child welfare applications.</small> </p> </div> <div class="CCSconcepts"> <ccs2012><small> <span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems → Decision support systems</strong>; • <strong>Applied computing → Computing in government</strong>;</small> </ccs2012> </div> <div class="classifications"> <div class="author"> <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>Algorithmic Accountability; Algorithmic Bias; Child Welfare Services; Decision-Support; Automated Decision Systems; Participatory Design</small></span> </div> <br> <div class="AcmReferenceFormat"> <p><small> <span style="font-weight:bold;">ACM Reference Format:</span> <br>Anna Brown, Alexandra Chouldechova, Emily Putnam-Hornstein, Andrew Tobin, and Rhema Vaithianathan. 2018. Toward Algorithmic Accountability in Public Services: A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-Making in Child Welfare Services. In <em>CHI Conference on Human Factors in Computing Systems Proceedings (CHI 2019), May 2019, Glasgow, Scotland UK.</em> ACM, New York, NY, USA 12 Pages. <a href="https://doi.org/10.1145/3290605.3300271" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3290605.3300271</a></small></p> </div> </div> </section> </section> <section class="body"> <section id="sec-2"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">1</span></span> INTRODUCTION</h2> </div> </header> <p>Algorithmic decision-making systems are increasingly being adopted by governments in an effort to improve and reform existing public service processes. Decisions about where to police [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0007" data-original-title="" title="" id="auto-BibPLXBIB00071">7</a>], whom to detain[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0028" data-original-title="" title="" id="auto-BibPLXBIB00282">28</a>], which child maltreatment allegations to investigate [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0020" data-original-title="" title="" id="auto-BibPLXBIB00203">20</a>], whom to grant access to permanent supportive housing [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0041" data-original-title="" title="" id="auto-BibPLXBIB00414">41</a>], and what level of unemployment benefit to provide [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0031" data-original-title="" title="" id="auto-BibPLXBIB00315">31</a>] are all being informed—if not performed—by algorithmic systems. These technologies and the opaque manner in which they are deployed have drawn tremendous criticism from researchers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0006" data-original-title="" title="" id="auto-BibPLXBIB00066">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0014" data-original-title="" title="" id="auto-BibPLXBIB00147">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0016" data-original-title="" title="" id="auto-BibPLXBIB00168">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0018" data-original-title="" title="" id="auto-BibPLXBIB00189">18</a>], policymakers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0032" data-original-title="" title="" id="auto-BibPLXBIB003210">32</a>], civil rights groups[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0011" data-original-title="" title="" id="auto-BibPLXBIB001111">11</a>], and the media[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0003" data-original-title="" title="" id="auto-BibPLXBIB000312">3</a>]. There is concern that data-driven algorithmic tools may be perpetuating discriminatory practices and having unintended consequences, all while operating outside the scope of traditional oversight and public accountability mechanisms.</p> <p>Several recent proposals have been put forth in an effort to establish good operating practices for promoting ‘algorithmic accountability’ [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0013" data-original-title="" title="" id="auto-BibPLXBIB001313">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0024" data-original-title="" title="" id="auto-BibPLXBIB002414">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0036" data-original-title="" title="" id="auto-BibPLXBIB003615">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0037" data-original-title="" title="" id="auto-BibPLXBIB003716">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0040" data-original-title="" title="" id="auto-BibPLXBIB004017">40</a>]. These all offer recommendations for what public agencies can do to enable stakeholders and the public to assess envisaged systems and engage in debate over whether their use is acceptable. Many of the proposals indicate that agencies should specifically seek to engage with affected communities—individuals who are most likely to be subject to, or impacted by, the deployment of algorithmic systems. This helps to ensure that the proposed system will meet the needs of those most directly affected by it, and bestows a kind of ‘license to operate’[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0040" data-original-title="" title="" id="auto-BibPLXBIB004018">40</a>].</p> <p>While public service agencies routinely engage affected communities in deliberative processes, these efforts are generally carried out on an ad hoc basis. The resulting findings remain internal to the given agency, and thus fall short of providing generalizable knowledge from which a broader understanding of effective design strategies could emerge. This paper seeks to take initial steps in bridging this gap.</p> <p>As part of a broader participatory design effort, we conducted workshops to learn about the concerns of affected communities in the context of child welfare services. These communities include families involved in the child welfare system, and social workers whose professional roles will be impacted by the introduction of algorithmic systems. The work presented here reflects a pilot study conducted with the further intent of designing (with the same participant communities) a blueprint to aid government agencies and data scientists in improving community comfort levels with algorithmic decision-making. Our study was designed to answer three central questions:</p> <ol class="list-no-style"> <li id="list1" label="(1)">How do people who are most likely to be subject to or affected by algorithmic decision-making feel about the deployment of such systems?<br></li> <li id="list2" label="(2)">What are the primary sources of community discomfort surrounding the development and deployment of such tools?<br></li> <li id="list3" label="(3)">What can researchers and designers do in the development and deployment stages to raise comfort levels among affected communities?<br></li> </ol> <p>In addition to this research article, we have produced an in-depth report on the study's findings intended for more general consumption. This report contains many more participant quotes and other information that could not be accommodated by the conference proceedings format. The report will be made available for download from the authors’ academic website.</p> </section> <section id="sec-3"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">2</span></span> BACKGROUND AND MOTIVATION</h2> </div> </header> <p>While we are not the first to explore issues of algorithmic accountability in the public sector, a distinguishing feature of our work is that it is directly motivated by ongoing efforts by members of the research team to develop and deploy algorithmic tools for use in child welfare. These tools are intended to inform service delivery and investigation decisions to better focus limited resources on the riskiest and neediest cases. This work has significant buy-in from agency leadership, and is intended to inform the next stages of algorithm design.</p> <p>The use of algorithmic tools in child welfare is a contentious issue. Several recent attempts at deploying algorithmic systems in child welfare have met with scathing criticism that ultimately resulted in their termination. In December 2017 the Illinois Department of Children and Family Services announced that they were terminating their predictive analytics program for reasons including the perception by DCFS staff that predictions were unreliable, issues with how the predictions were communicated, data quality, and questions surrounding the initial procurement process [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0021" data-original-title="" title="" id="auto-BibPLXBIB002119">21</a>]. Earlier that same year the County of Los Angeles Office of Child Protection released a report examining the use of predictive analytics in assessing child safety and risk in which they cite the black-box nature of proprietary tools and high false positive and false negative error rates as factors driving the county's decision to terminate their project. Studies such as our own can serve to inform more effective and accountable uses of algorithmic systems by helping to identify affected community concerns during the design phase.</p> <p>Algorithmic systems of the kind we consider here are in many cases already governed by existing data protection regulations such as those articulated in the EU General Data Protection Regulation (GDPR) and Canada's Personal Information Protection and Electronic Documents Act (PIPEDA). Such regulations are primarily intended to govern private sector data use and commonly offer exceptions for public sector use. To the extent that they apply in a given instance, they provide data subject rights, and dictate necessary criteria for data use and storage. Necessary, however, is not sufficient.</p> <p>Algorithmic systems that are deployed in full compliance with the existing regulation may nevertheless fail to have so-called “license to operate”, also known as “social license”. In the industrial context, this refers to community and stakeholder acceptance of a company's operating procedures business practices. As Shah [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0040" data-original-title="" title="" id="auto-BibPLXBIB004020">40</a>] argues, without such licence from the public, the promise of algorithmic systems for promoting positive social change may fail to be fully realized. Better understanding affected community concerns is an important step in building toward trust and social license.</p> <p>Our study is informed by work characterizing the structure of organizational justice perceptions. Colquitt [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0012" data-original-title="" title="" id="auto-BibPLXBIB001221">12</a>] identifies four central components of justice perceptions: distributive (how equitably resources are distributed); procedural (the fairness, consistency and accuracy of the decision process); interpersonal (feelings that one is treated with due respect); and informational (perceived truthfulness and comprehensiveness of justifications provided). Procedural justice is a particularly important concept, as it has been found to mitigate the relationship between outcome favorability and support for decisions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0008" data-original-title="" title="" id="auto-BibPLXBIB000822">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0009" data-original-title="" title="" id="auto-BibPLXBIB000923">9</a>]. Outcomes in the child welfare system are often unfavorable to families. Understanding how algorithmic systems affect perceptions of procedural justice is therefore central to design and deployment considerations.</p> <p>We build upon a growing body of work in the HCI community that has explored perceptions of fairness and procedural justice in algorithmic systems [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0001" data-original-title="" title="" id="auto-BibPLXBIB000124">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0005" data-original-title="" title="" id="auto-BibPLXBIB000525">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0025" data-original-title="" title="" id="auto-BibPLXBIB002526">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0026" data-original-title="" title="" id="auto-BibPLXBIB002627">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0027" data-original-title="" title="" id="auto-BibPLXBIB002728">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0035" data-original-title="" title="" id="auto-BibPLXBIB003529">35</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0044" data-original-title="" title="" id="auto-BibPLXBIB004430">44</a>]. Notably, the recent work of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0044" data-original-title="" title="" id="auto-BibPLXBIB004431">44</a>] reports on a series of workshops that presented participants with scenarios based on internet-related products and services. The scenarios were constructed to reflect different forms of discrimination, stereotyping and exclusion on the basis of race, sex, or geographic location. Study participants were engaged in extensive discussion of their reactions to the instances of “algorithmic bias”. The authors specifically recruited participants from traditionally marginalized populations, including racial and ethnic minorities and persons of low socioeconomic status. This focussed the study on those populations most likely to be affected by algorithmic bias in internet services.</p> <p>Several recent studies have investigated public [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0017" data-original-title="" title="" id="auto-BibPLXBIB001732">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0039" data-original-title="" title="" id="auto-BibPLXBIB003933">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0043" data-original-title="" title="" id="auto-BibPLXBIB004334">43</a>] and practitioner [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0029" data-original-title="" title="" id="auto-BibPLXBIB002935">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0042" data-original-title="" title="" id="auto-BibPLXBIB004236">42</a>] perspectives on the use of algorithmic systems in the context of public sector decision making. Scurich and Monahan [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0039" data-original-title="" title="" id="auto-BibPLXBIB003937">39</a>] and Grgić-Hlača et&nbsp;al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0017" data-original-title="" title="" id="auto-BibPLXBIB001738">17</a>] investigate the narrower question how the inclusion of particular features influences public perceptions of justice and fairness. Wang [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0043" data-original-title="" title="" id="auto-BibPLXBIB004339">43</a>] studies public perceptions of procedural justice more broadly in the context of the criminal justice system. These studies of public perception tend to focus on nationally representative samples of the general population, or rely on MTurk workers. In both cases, the participant samples are neither representative of nor targeted towards populations that have direct experience with the public system in question. Participants in our study frequently invoked personal experiences with the child welfare system, which suggests that such first-hand experience is important to informing perceptions. Furthermore, we are not aware of any prior studies that, like our own, involve both affected members of the public and practitioners within the system.</p> </section> <section id="sec-4"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">3</span></span> METHODOLOGY</h2> </div> </header> <p>The present study reflects just one element of a broader participatory design effort to improve child welfare outcomes through the use of data-driven algorithmic tools. Participatory design methodologies favor a human-centred approach that engages the participation of people most likely to be affected by the services and policies being designed. Within participatory design sits an established set of practices, tools and techniques to explore and understand the ‘lived experience’ of everyday people [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0038" data-original-title="" title="" id="auto-BibPLXBIB003840">38</a>]. These methods have been successfully applied to address technological design challenges in the public sector. Notably, research [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0004" data-original-title="" title="" id="auto-BibPLXBIB000441">4</a>] commissioned by the Data Futures Partnership formed the basis of national guidelines for New Zealand organizations to obtain social license for data use [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0034" data-original-title="" title="" id="auto-BibPLXBIB003442">34</a>].</p> <p>Participatory design is characterised by generative, experiential and action-based methods that place emphasis on embodied learning and explorations into future scenarios [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0022" data-original-title="" title="" id="auto-BibPLXBIB002243">22</a>]. To study community perspectives, we adopted a methodology where:</p> <ul class="list-no-style"> <li id="list4" label="•">Meaningful conversations are facilitated with affected communities using scenarios that model a real life situation while exploring data use variables of interest to the study;<br></li> <li id="list5" label="•">Participants are invited to deliberatively demonstrate their levels of comfort with scenarios using a commonsense trust/benefit matrix;<br></li> <li id="list6" label="•">Participants are invited to identify what would increase their comfort in the data use and sharing scenarios explored; and<br></li> <li id="list7" label="•">Participants identify and prioritize the themes and concerns that they consider most important to increasing comfort.<br></li> </ul> <p>Our study received full IRB approval from all member institutions involved in the human subjects research and primary data analysis. Study participants were provided with an informed consent form at the beginning of the workshop session. In devising the study we consulted with ethics specialists at the local child welfare agencies in order to ensure that our design minimized the risk of unintended harm to study participants, and facilitated an environment in which participants felt comfortable expressing their beliefs and opinions.</p> <section id="sec-5"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Participants</h3> </div> </header> <p>Four participant samples were recruited from a single jurisdiction in a mid-sized US county. The three main groups included in the full analysis are:</p> <ul class="list-no-style"> <li id="list8" label="•"><strong>Families</strong> (<em>n</em> = 18) Two workshops of family participants were recruited through community providers. A condition of recruitment was that peer support people would attend and participate in the discussions. Their responses have been included in the sample and the findings. Four of the eighteen participants in the families workshops were peer support people.<br></li> <li id="list9" label="•"><strong>Frontline Providers</strong> (<em>n</em> = 38) Three workshops included child welfare worker participants who were in regular contact with families. These workshops were variably peopled by family support people, peer support people and case workers. These participants were recruited through provider networks by the relevant local state agency.<br></li> <li id="list10" label="•"><strong>Specialists</strong> (<em>n</em> = 11) Specialist providers in the sample above were a subgroup who attended the provider workshop whose roles were identified mostly as not involving regular contact with families. Types of specialists included data workers, supervisors and office managers.<br></li> </ul> <p>A fourth sample of <strong>Prototype Specialists</strong> included <em>n</em> = 16 participants from the city's child protective services head office employed in roles not involving regular contact with families. These participants took part in an earlier prototype of the workshop scenarios, and their perspectives have been included in the findings. Since the prototype scenarios were slightly different from those that all other participants responded to, responses from this group have been filtered to remove any that were attributable to scenario differences.</p> <p>Family participants were recruited through community providers whose peer support staff approached families with invitations to participate in the workshops. These participants were offered childcare for the full duration of the workshop, a light pizza dinner, and modest compensation for their time. Frontline providers and specialists were recruited through the local human services agency and by community providers. Most of the participants were women, which in the frontline provider and specialist groups is consistent with the over-representation of women in social work and related professions.</p> <p>Recruitment for one of the workshops specifically focused on participants identifying as persons of colour. While prior studies have found significant differences in perceptions of procedural justice across racial and ethnic groups [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0019" data-original-title="" title="" id="auto-BibPLXBIB001944">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0033" data-original-title="" title="" id="auto-BibPLXBIB003345">33</a>], the responses given in our study were similar overall. The main notable differences arose when participants described their personal experiences as persons of color affected by the child welfare system.</p> </section> <section id="sec-6"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Workshops</h3> </div> </header> <p>We developed a series of progressive scenarios designed to explore variations of interest to the research team concerning the use of algorithmic decision-making by public service agencies. Specifically, the scenarios varied across three distinct dimensions:</p> <ul class="list-no-style"> <li id="list11" label="•"><strong>Decision making framework</strong>: We varied whether the decision was made by a human (human), by a human assisted by an algorithm (algorithm-assisted), or by an automated algorithmic decision-making system (algorithmic).<br></li> <li id="list12" label="•"><strong>Proactive vs. reactive</strong>: We considered both <em>reactive</em> scenarios, where the decision was prompted by a report made to child welfare services, and <em>proactive</em> scenarios, where an individual becomes eligible for support or services on the basis of an algorithmic assessment.<br></li> <li id="list13" label="•"><strong>Data sources</strong>: We varied the breadth of data that was used to inform the decision. Most narrowly, we considered scenarios where only the family's own child welfare data factored into the decision. Later scenarios introduced child welfare data from associates of the family, data from other administrative systems (criminal justice), and local community data not specific to the family (e.g., neighbourhood).<br></li> </ul> <p>The scenarios were tested and refined via a prototype audience of US child welfare specialists to ensure that they were understandable and relatable. The structure of the final scenarios is summarized below.<a class="fn" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#fn1" id="foot-fn1" data-original-title="" title=""><sup>1</sup></a> </p> <ul class="list-no-style"> <li id="list14" label="•"><strong>Scenario 1a</strong><br><em>Reactive + human decision-making</em><br> Presents a child welfare decision being made by an intake worker in reaction to a call from a member of the public. There is no mention of personal data or a computer tool being involved in making the decision.<br></li> <li id="list15" label="•"><strong>Scenario 1b</strong><br><em>Reactive + algorithm-assisted decision-making</em><br> Presents a child welfare action that is reactive, and a decision that is made by a human assisted by an algorithm.<br></li> <li id="list16" label="•"><strong>Scenario 1c</strong><br><em>Reactive + algorithmic decision-making + using family's child welfare data</em><br> Presents a child welfare action that is reactive, and a decision that is made by an algorithm including associative data (child welfare investigation).<br></li> <li id="list17" label="•"><strong>Scenario 1d(i), (ii) &amp; (iii)</strong><br><em>Proactive + algorithmic decision-making + using family's child welfare data</em><br> Presents a child welfare action that is proactive, and a decision to offer services that is made by an algorithm — with three different ways of communicating this decision to the family.<br></li> <li id="list18" label="•"><strong>Scenario 1e</strong><br><em>Reactive + algorithmic decision-making + using administrative data beyond child welfare</em><br> Presents a child welfare action that is reactive, and a decision that is made by an algorithm including associative data (criminal justice data).<br></li> <li id="list19" label="•"><strong>Scenario 1f</strong><br><em>Proactive + algorithmic decision-making + using administrative and community data</em><br> Presents a child welfare action that is proactive, and a decision that is made by an algorithm including non-associative data (criminal records, neighborhood, age).<br></li> </ul> <p>Figure <a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#fig1">1</a> displays all six of the scenarios as they were shown to the main workshop participants (those shown to the prototype sample were slightly different). The first scenario outlines a situation that is constructed to be most familiar to the participants’ lived experience. This familiarity and engagement is then leveraged to incrementally introduce scenario elements that participants have not experienced, but which they can understand and respond to in the context of the evolving narrative. </p><figure id="fig1"> <img src="./paper-chi-algorithm-4_files/chi2019-41-fig1.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 1:</span> <span class="figure-title">All six scenarios as presented to study participants (see Section <a class="sec" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#sec-4">3</a> for more information). During the workshops each participant was provided with a printout of the scenarios with one scenario appearing per page. Participants were instructed to not advance to later scenarios until the current one had been fully discussed.</span> </div> </figure> <p></p> <p>The participants at each workshop were split into two groups, each of which was guided by a research facilitator. All participants were provided with a print-out of the scenarios for their reference throughout the discussion. Participants were instructed to not view later scenarios before the current one was finished being discussed. Each scenario was read out by the group's research facilitator. Certain parts of each scenario description were bolded for emphasis. These corresponded to the specific decision and action that was taken in the scenario. However, participant perspectives could be—and were—influenced by any part of the text.</p> <p>Upon being presented with the given scenario, participants were then asked to place an identifier token on a <em>comfort board</em> to indicate their perceived level of ‘benefit’ and ‘trust’ in the situation. Here, ‘benefit’ refers to the amount of benefit they see arising as a result of the decision or action taken; and ‘trust’ refers to how much trust they have in the decision, action, or system actors. Participants were asked to reflect on the scenarios from the the perspective of an affected parent (for later scenarios, grandparent). Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#fig2">2</a> shows a photo of the comfort board for one of the groups in response to scenario 1b.</p> <p>After taking positions on the board, participants were engaged in a facilitated conversation about their perceived levels of comfort with the child welfare decision-making processes and actions in the given scenario. They were then asked to identify what, if anything, would increase their levels of comfort. Upon completing the final scenario the participants were engaged in a broader discussion aimed at synthesizing and prioritizing overarching concerns and recommendations for system design. </p><figure id="fig2"> <img src="./paper-chi-algorithm-4_files/chi2019-41-fig2.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 2:</span> <span class="figure-title">Comfort board for one of the workshop participant groups in response to scenario 1b. Each letter token on the board is a unique participant identifier that is retained across all of the scenarios. The gradient shading on the comfort board is intended to reflect different levels of overall ‘comfort’ with the scenario. Lighter colors correspond to greater overall comfort.</span> </div> </figure> <p></p> </section> <section id="sec-7"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Data collection and analysis</h3> </div> </header> <p>Three sets of data were collected. Data set 1 was composed of photographs of the positions of participants’ counters comfort map in response to each scenario. Data set 2 consisted of verbal comments made by workshop participants which were noted verbatim by facilitators and coded according to which scenario they applied to. Data set 3 was composed of summative comments written by participants on sticky notes at the conclusion of each workshop in answer to the question, “Considering all of the scenarios we've discussed, what are the most important things that are needed to increase your comfort with the use of data and computer tools in social welfare decision-making?” Participants were facilitated to sort these comments into thematic groups on a sheet, give each theme a label, and prioritize themes according to their relative importance.</p> <p>At the conclusion of workshops, Data set 1 (visual comfort maps) was manually translated into derived comfort map matrices (such as shown in Figure <a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#fig2">2</a>) that could be examined to identify patterns across scenarios and participants. Data set 2 (verbatim participant comments) was transcribed by each researcher (individually) into a spreadsheet which could be also be filtered by scenario and participant type. Individual researchers used grounded theory to inductively code, conceptualize and categorize data from their workshops, and then came together to inductively assess common and unique findings. At this point, data set 3 (themes self-identified by participants being most important) was inductively analyzed by the research team, and these codes, concepts and categories were compared with those resulting from team analysis of data set 2. Finally, the research team drew theoretical conclusions drawing on all data sets, with the lead researcher reporting these conclusions and evidencing them with verbatim comments and visual comfort maps.</p> </section> <section id="sec-8"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Study limitations</h3> </div> </header> <p>Before proceeding to our main findings, we highlight several limitations of our study that are important to bear in mind while interpreting the results. First, the recruitment strategy was one of convenience sampling. This means that our study sample is almost surely not representative of the full population of individuals affected by the county's child welfare system. Second, the study was conducted in a US county that is already in the process of adopting algorithmic decision-support tools in the child welfare context. The specialist subgroup in particular consists of child welfare workers who may already be familiar with the county's approach. This group is thus likely to be much more informed about the relevant issues compared to workers in other jurisdictions. Lastly, the scenarios we explored are all narrowly focused on algorithmic decision making in the US child welfare system. Our findings cannot directly speak to concerns or experiences of affected communities involved in other systems such criminal justice, homelessness, or unemployment services.</p> <p>We also concede that a full participatory design framework would ideally entail designers, families, frontline providers and specialists all gathering together on an equal basis to learn from each other. For this study, however, we accepted guidance from ethics specialists in the local child welfare agencies that there was a risk of many forms of bias from basing our pilot on such an approach. This was a an especially significant concern with respect to families, whose responses, it was feared, may be unduly influenced or censored in a mixed-group approach. Indeed, we included peer support in the family workshops precisely to further facilitate greater freedom of dialogue among family participants.</p> </section> </section> <section id="sec-9"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">4</span></span> FINDINGS</h2> </div> </header> <p>In this section we summarize several of the major themes that emerged from the workshops. Overall, participants tended towards expressing low levels of comfort across the range of scenarios. Reasons for lower comfort were found to fall under three broad themes: (i) system-level concerns; (ii) scenario-specific concerns about how data and algorithms are used in decision-making; and (iii) concerns about how agencies and service providers relying on algorithmic tools communicate and interact with families. Reasons for higher comfort were more difficult to generalize and tended to be particular to the scenario and participant type. Reasons for higher comfort given in Scenarios 1d(i) and 1e had the most generalizable themes. These included (i) improved access to services; and (ii) the nature and relevance of the data going into the risk score. We elaborate on each of these themes below, highlighting notable differences in how they were expressed by the different participant groups. This section speaks most directly to the first two central questions posed at the outset of the paper: how do affected communities feel, and what are the primary sources of comfort and discomfort?</p> <section id="sec-10"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> System-level concerns were the most common reasons given for low comfort in algorithm-assisted and algorithmic decision-making</h3> </div> </header> <p>Participants across all of the represented groups — families, frontline providers and specialists — made frequent reference to ‘the system’ and ‘government’ to explain their reasons for positions of low comfort across the different scenarios. In discussion these terms could variously refer, either generally or specifically, to local government or to wider US government agencies (most commonly in child welfare, but also criminal justice, health, education and and social welfare), and to the people, decisions, actions and behaviours of those agencies. These point to low baseline perceptions of procedural and interpersonal justice of the child welfare system as a whole.</p> <p>Families often referred to negative experiences in their own lives, and reflected on the oppositional nature of the system, saying, for instance, “It's been me versus the system” (Family - 1a). Most family participants had low perceptions of trust in the decisions made, and low expectations of the benefits that the system could provide.</p> <quote> <p>“They would look at me more because I had previous experience than because they wanted to help me with my daughter.” (Family - 1c)</p> </quote> <p>Even in settings where participants saw a clear benefit, system-level factors were cited as primary reasons for low trust, and thus low overall comfort.</p> <quote> <p>“I see the benefit — I'm the grandma and we need help, my daughter and the young father. I still don't trust the system, though. I'd look for the help elsewhere.” (Specialist - 1d)</p> </quote> <p>Frontline providers and specialists expressed concern regarding negative system approaches that placed too much emphasis on the ‘deficits’ and ‘risks’ of a case. Providers referred to their professional or personal experiences of ‘the system’ in describing what they believed to be a ‘deficit-based’ approach. Specialists often explained their positions of low comfort as a response to the language and terminology used in the scenarios. For instance, the term ‘investigation’ was commonly perceived as a clear example of negative language, as were technical terms such as ‘statistical tool’, ‘risk score’, and ‘high risk’.</p> <quote> <p>“It seems like a deficit model – let's weigh up all the dirty things in your life, nothing good though.” (Provider - 1d)</p> </quote> <quote> <p>“‘Investigation’ says that you've been judged already”.</p> <p>&nbsp; (Specialist - 1a)</p> </quote> </section> <section id="sec-11"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> All groups raised concerns about potential bias on the part of case workers involved in the decision process, as well as bias present in the data or the algorithm</h3> </div> </header> <p>Many participants voiced concerns about bias and discrimination on the basis of race, ethnicity, gender, poverty status, neighborhood, and urban/rural location. Families questioned how and whether the computer tool takes race or ethnicity into account, and whether the data itself might be biased due to selective sourcing. Personal experiences of racial bias were often cited as reasons for low comfort.</p> <quote> <p>“The system here in America just lets us down, especially if you are Black.” (Family-1b)</p> </quote> <quote> <p>“Knowing what ‘the system’ has done to people of colour, I'm coming from a position of very low trust. As a black mother, I have [an advanced degree] and a career and I still have that fear of a system that is checking on that, on me as being black. That white mother sitting next to me, they won't investigate her like they will me. Knowing that my black friends’ babies were drug tested in hospital, it's just a fact that I'm treated a certain way because of who I am.” (Specialist - 1b)</p> </quote> <quote> <p>“How honest are we allowed to be? Most of our systems were not made for people of colour, or by people of colour, or have people of colour in them.”. (Specialist - 1a)</p> </quote> <p>Where a data-driven algorithm was used in the decision-making process, frontline providers raised concerns about data fidelity and bias, as well as its impact on case worker judgment. Providers questioned the extent to which outdated historical data might be given undue weight in decisions about a new situation, and whether old referrals with no negative findings should be taken into consideration. They also noted that selective sourcing and differential availability of data could result in policies that disproportionately impacted low socioeconomic status populations.</p> <quote> <p>“My neighbour might be shooting up heroin and their six year old is out in the street, but they have private insurance so their records aren't part of this system. The computer tool is only capturing people who have to use public health so there's a bias to poorer people in the system.” (Provider - 1b)</p> </quote> </section> <section id="sec-12"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Participants questioned whether a statistical model could adequately account for all relevant decision elements, and emphasized the need for a human in the loop approach</h3> </div> </header> <p>Family participants mostly expressed general concerns about ‘human versus computer’ capabilities. Some participants were specifically concerned that data-driven algorithmic tools would not be able to produce risk scores that accurately accounted for salient aspects of their circumstances.</p> <quote> <p>“A computer cannot understand context. My son has autism — how does the data account for this?” (Family - 1b)</p> </quote> <p>Frontline providers and specialists emphasized the need to supplement the algorithmic assessments with a more in-depth look at the family's situation and history. They were vocal in questioning decision-making systems that relied on algorithms alone, stressing that child welfare decisions require an understanding of context that is only possible through human interpretation and contact.</p> <quote> <p>“The score should be a flag rather than a definitive ‘go’. It needs to be approached with curiosity: Where are you at? What are you facing? What are your needs? Would you benefit from home visits, more community? Help put it back together. If child welfare was just a score we wouldn't be sitting here.” (Provider - 1b)</p> </quote> <quote> <p>“Use data without removing human decision-making.” (Specialist - 1c)</p> </quote> <p>The idea of fully automated algorithmic decision systems was met with low perceptions of procedural justice. While algorithmic risk scores were perceived as potentially helpful as a starting point, they were generally deemed to be an inadequate basis for ultimate decision-making.</p> </section> <section id="sec-13"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Participants wanted more information on how the algorithm weighs different factors, and the ability to dispute the score</h3> </div> </header> <p>Many of the participants expressed discomfort with the black-box manner in which the algorithmic tool was presented. Specifically, they wanted to have more information on how the tool was constructed, and the weights given to different factors included in the model. This indicated low perceptions of informational justice in the algorithm-assisted and algorithmic decision-making scenarios.</p> <quote> <p>“How do you determine if a person has a record? How did the data get scored? What is the process for deciding the score?” (Provider - 1b)</p> </quote> <quote> <p>“A risk score from 1-10? What is the criteria? I want to know that there is consistency.” (Specialist - 1b)</p> </quote> <p>Echoing earlier concerns about a ‘deficits-based’ approach, participants also questioned whether ‘positive’ data about families was taken into account, or if the model weighed only ‘negative’ factors associated with increased risk.</p> <p>The black-box nature of the algorithmic tools also led to concerns surrounding recourse and contestability. Participants wanted to be able able to dispute input data and risk assessments that they disagree with.</p> <quote> <p>It's like with credit scores, the Fair Credit [Reporting] Act gives options to contest it if you disagree with data, an official way to fight to get things removed if reported erroneously.” (Provider - 1b)</p> </quote> </section> <section id="sec-14"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Even potentially beneficial decisions resulted in discomfort due to concerns about how and whether risk information was communicated to families and case workers</h3> </div> </header> <p>All participant groups expressed concern at a perceived lack of supportive communication with families across the range of scenarios.</p> <quote> <p>“It's all about communication — it starts here and everything else is part of this.’ (Family-1d)</p> </quote> <quote> <p>“The computer tool is the ‘why’ to approach a family NOT the ‘way’ to approach a family” (Provider - 1c)</p> </quote> <p>This became especially pronounced in the later scenarios such as 1d. Families and providers perceived the explicit mention of a statistical tool as being antithetical to engendering trust with the young mother in the scenario.</p> <quote> <p>“I see high benefit but low trust. If [the services offered are] voluntary then surely it's good. She's a kid so it's not a bad thing. Often you don't know what's out in your community. But I don't like her being told she was flagged by a statistical tool.” (Provider - 1d)</p> </quote> <p>While many saw significant benefit in the proactive offering of services triggered by the tool, this was often outweighed by deep distrust resulting from the communication between the nurse and the mother. Many participants described the explicit mention of risk levels in 1d(ii) and 1d(iii) as coercive and fear-based.</p> <quote> <p>“It's the way they're doing it—I don't feel you want to do that to anyone. You can provide some of that information without this—almost implying that ‘if you don't take this your child will be taken away’.” (Family - 1d)</p> </quote> <p>Interestingly, scenario 1d(i) had the greatest divergence in comfort levels across the different participant groups. Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#fig3">3</a> shows derived plots of the comfort maps for 1d(i) and 1d(iii) for all three main participant groups. Specialists started out expressing uniformly very high comfort. Unlike families and frontline providers, they did not perceive a serious issue with the general mention of a ‘statistical tool’. Specialists expressed much lower levels of comfort in scenarios 1d(ii) and 1d(iii), taking issue with the manner in which risk was being communicated. </p><figure id="fig3"> <img src="./paper-chi-algorithm-4_files/chi2019-41-fig3.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 3:</span> <span class="figure-title">Derived comfort maps for scenarios 1d(i) (<strong>left</strong>) and 1d(iii) (<strong>right</strong>).<a class="fn" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#fn2" id="foot-fn2" data-original-title="" title=""><sup>2</sup></a> Darker colours indicate more participants in the given cell.</span> </div> </figure> <p></p> <p>There was also disagreement on the perceived benefit of the case workers in the scenario having knowledge of the risk score and the risk factors involved. By some, this knowledge was viewed as critical to equipping workers to take informed actions.</p> <quote> <p>“Social welfare workers need to be equipped to explain, ‘this is why we're doing this, these are the factors that made us concerned’.” (Specialist - 1c)</p> </quote> <quote> <p>“Does the social worker know about the risk score? If they don't, what if they are walking into a home with domestic violence potential without knowing?” (Specialist - 1c)</p> </quote> <p>However, there was also concern that by knowing the score in advance social workers may be less able to fairly evaluate the situation.</p> <quote> <p>“But as a social worker you should not know the risk score — you need to be able to give a fair assessment.” (Specialist - 1c)</p> </quote> </section> </section> <section id="sec-15"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">5</span></span> DISCUSSION</h2> </div> </header> <p>In this section we turn to the final question raised at the outset of the paper: What can researchers and designers working in partnership with public service agencies do in the development and deployment stages to raise comfort levels among affected communities? These recommendations are drawn from both scenario-specific and general discussions with workshop participants.</p> <section id="sec-16"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Address system-level concerns.</h3> </div> </header> <p>The introduction of an algorithmic tool does not appear to improve trust or perceptions of procedural justice on its own. On the contrary, the very same system-level concerns that pervaded the discussion were readily projected onto the algorithmic decision-making scenarios. If these concerns are not addressed through complementary policy changes, even the best-conceived algorithmic tools and intervention strategies are likely to be met with distrust and low comfort.</p> </section> <section id="sec-17"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Weigh positive and negative data in algorithmic decision-making.</h3> </div> </header> <p>The development of predictive analytics for use in human services follows the tradition of ‘risk assessment’ wherein an individual's likelihood of an adverse outcome is assessed using a set of predictive ‘risk factors’. This framing focuses attention on predicting a negative outcome (‘failure’) based on negative inputs that capture ‘deficits’ instead of ‘strengths’. There is concern that such approaches risk anchoring workers to a disproportionately negative view of the situation, which may in turn drive negative actions. While capturing ‘strengths’ in some instances requires the collection of additional data, simpler design changes may also be effective. Modeling success instead of failure may be a matter of recoding the outcome variable of interest, or simply reporting the likelihood of not-failure. Likewise, certain deficit variables or risk factors can be inverted into more positive variables that capture strengths instead. These simple modifications would produce a model that is mathematically equivalent to the original, but which may be perceived and responded to very differently by users and affected individuals.</p> </section> <section id="sec-18"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Convey how making use of data and algorithms leads to improved family and process outcomes.</h3> </div> </header> <p>Participants generally agreed that algorithmic tools that systematically identify and flag important risk factors such as criminal history provide significant benefit over less systematic approaches. They also viewed the proactive offering of services on the basis of algorithmic assessments to be beneficial to families in need. However, it was unclear to participants whether there were any other benefits to algorithmic decision-making systems. It was also unclear that any such benefits would outweigh the concerns raised by such systems. As one frontline provider put it “How is a ‘computer tool’ better than ‘no computer tool’?” Participants were specifically interested in evidence that algorithmic systems would produce better child welfare outcomes than traditional decision-making approaches. Outcomes of particular interest include increased child safety, decreased disparities, and greater access to strengths-based family support.</p> </section> <section id="sec-19"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Facilitate supportive communication and positive relationships between child welfare workers and families.</h3> </div> </header> <p>Even if an algorithmic back-end is triggering particular actions, those actions are still typically being carried out by child welfare workers through direct interactions with families. Families need to know they can trust frontline providers to act in their best interests. In turn, frontline providers need a clear understanding of how algorithmic decision-making can inform the formation of positive and safe professional relationships with families.</p> <p>Throughout the workshops, participants raised numerous concerns surrounding the opacity of algorithmic decision-making as described in the scenarios. These concerns may be viewed as obstacles to the ultimate goal of forming supportive relationships. For instance, participants indicated that they would feel more comfortable if they had: (i) knowledge of what data was being used and how; (ii) knowledge of how the score is used, shared, and stored; (iii) the ability to contest inaccurate data; (iv) knowledge of how different factors are weighed in the risk score; and (v) the criteria for acting upon a risk score. However, we caution that simple disclosure of this information is unlikely to have much effect on promoting relationship-building. More research is needed to understand how different elements of algorithmic systems affect perceptions of interpersonal and informational justice.</p> </section> </section> <section id="sec-20"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">6</span></span> CONCLUSION</h2> </div> </header> <p>The fairness and interpretability of algorithms deployed in consequential decision-making settings has received significant attention in recent years. This has led to a numerous proposals for auditing strategies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0015" data-original-title="" title="" id="auto-BibPLXBIB001546">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0045" data-original-title="" title="" id="auto-BibPLXBIB004547">45</a>], and the development of models that are constructed to be in some sense ‘fair’ by design [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0002" data-original-title="" title="" id="auto-BibPLXBIB000248">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0023" data-original-title="" title="" id="auto-BibPLXBIB002349">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0030" data-original-title="" title="" id="auto-BibPLXBIB003050">30</a>]. Existing work has even explored the algorithmic bias properties of tools deployed in the child welfare system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#BibPLXBIB0010" data-original-title="" title="" id="auto-BibPLXBIB001051">10</a>]. Our study suggests that these technical solutions are in and of themselves insufficient for ensuring that the resulting algorithmic systems are perceived as fair and just. Perceptions of algorithmic fairness are heavily influenced by perceptions of procedural and interpersonal justice of the child welfare system as a whole. Effective design strategies must therefore consider how technological solutions can be designed to work in concert with complementary policy changes to impact community perceptions of system justice.</p> </section> </section> <section class="back-matter"> <section id="sec-21"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> ACKNOWLEDGMENTS</h2> </div> </header> <p>The authors thank SIGKDD for the seed funding for this project provided through their KDD Impact Program. We also thank the anonymous referees for their many excellent comments and helpful suggestions for improving the manuscript.</p> </section> <section id="ref-001"> <header> <div class="title-info"> <h2 class="page-brake-head"><span class="nav-open" id="nav-open" onclick="openNav(this)" tabindex="0" title="navigate to">REFERENCES</span></h2> </div> </header> <ul class="bibUl"> <li id="BibPLXBIB0001" label="[1]" value="1">Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian&nbsp;Y Lim, and Mohan Kankanhalli. 2018. Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda. In <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>. ACM, 582. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 1" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000124" aria-label="citation 1 reference 1">citation 1</option></select></li> <li id="BibPLXBIB0002" label="[2]" value="2">Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. 2018. A reductions approach to fair classification. <em><em>arXiv preprint arXiv:1803.02453</em></em>(2018). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 2" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000248" aria-label="citation 1 reference 2">citation 1</option></select></li> <li id="BibPLXBIB0003" label="[3]" value="3">Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine Bias. (2016). <a class="link-inline force-break" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 3" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000312" aria-label="citation 1 reference 3">citation 1</option></select></li> <li id="BibPLXBIB0004" label="[4]" value="4">Toi Aria. 2017. Our data, our way. <a class="link-inline force-break" href="https://trusteddata.co.nz/massey_our_data_our_way.pdf">https://trusteddata.co.nz/massey_our_data_our_way.pdf</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 4" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000441" aria-label="citation 1 reference 4">citation 1</option></select></li> <li id="BibPLXBIB0005" label="[5]" value="5">Reuben Binns, Max Van&nbsp;Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, and Nigel Shadbolt. 2018. ’It's Reducing a Human Being to a Percentage’: Perceptions of Justice in Algorithmic Decisions. In <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>. ACM, 377. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 5" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000525" aria-label="citation 1 reference 5">citation 1</option></select></li> <li id="BibPLXBIB0006" label="[6]" value="6">Robert Brauneis and Ellen&nbsp;P Goodman. 2017. Algorithmic transparency for the smart city. (2017). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 6" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00066" aria-label="citation 1 reference 6">citation 1</option></select></li> <li id="BibPLXBIB0007" label="[7]" value="7">Sarah Brayne. 2017. Big data surveillance: The case of policing. <em><em>American Sociological Review</em></em>82, 5 (2017), 977–1008. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 7" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00071" aria-label="citation 1 reference 7">citation 1</option></select></li> <li id="BibPLXBIB0008" label="[8]" value="8">Joel Brockner and Batia Wiesenfeld. 2005. How, when, and why does outcome favorability interact with procedural fairness?(2005). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 8" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000822" aria-label="citation 1 reference 8">citation 1</option></select></li> <li id="BibPLXBIB0009" label="[9]" value="9">Joel Brockner and Batia&nbsp;M Wiesenfeld. 1996. An integrative framework for explaining reactions to decisions: interactive effects of outcomes and procedures.<em><em>Psychological bulletin</em></em>120, 2 (1996), 189. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 9" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000923" aria-label="citation 1 reference 9">citation 1</option></select></li> <li id="BibPLXBIB0010" label="[10]" value="10">Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema Vaithianathan. 2018. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. In <em>Conference on Fairness, Accountability and Transparency</em>. 134–148. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 10" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001051" aria-label="citation 1 reference 10">citation 1</option></select></li> <li id="BibPLXBIB0011" label="[11]" value="11">Coalition. 2018. The use of pretrial risk assessment instruments: A shared statement of civil rights concerns. <a class="link-inline force-break" href="http://civilrightsdocs.info/pdf/criminal-justice/Pretrial-Risk-Assessment-Full.pdf">http://civilrightsdocs.info/pdf/criminal-justice/Pretrial-Risk-Assessment-Full.pdf</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 11" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001111" aria-label="citation 1 reference 11">citation 1</option></select></li> <li id="BibPLXBIB0012" label="[12]" value="12">Jason&nbsp;A Colquitt. 2001. On the dimensionality of organizational justice: A construct validation of a measure.<em><em>Journal of applied psychology</em></em>86, 3 (2001), 386. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 12" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001221" aria-label="citation 1 reference 12">citation 1</option></select></li> <li id="BibPLXBIB0013" label="[13]" value="13">Nicholas Diakopoulos. [n. d.]. Algorithmic-Accountability: the investigation of Black Boxes. ([n. d.]). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 13" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001313" aria-label="citation 1 reference 13">citation 1</option></select></li> <li id="BibPLXBIB0014" label="[14]" value="14">Virginia Eubanks. 2018. <em>Automating inequality: How high-tech tools profile, police, and punish the poor</em>. St. Martin's Press. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 14" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00147" aria-label="citation 1 reference 14">citation 1</option></select></li> <li id="BibPLXBIB0015" label="[15]" value="15">Michael Feldman, Sorelle&nbsp;A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015. Certifying and removing disparate impact. In <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. ACM, 259–268. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 15" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001546" aria-label="citation 1 reference 15">citation 1</option></select></li> <li id="BibPLXBIB0016" label="[16]" value="16">Andrew&nbsp;Guthrie Ferguson. 2017. <em>The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement</em>. NYU Press. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 16" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00168" aria-label="citation 1 reference 16">citation 1</option></select></li> <li id="BibPLXBIB0017" label="[17]" value="17">Nina Grgić-Hlača, Elissa&nbsp;M Redmiles, Krishna&nbsp;P Gummadi, and Adrian Weller. 2018. Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction. <em><em>arXiv preprint arXiv:1802.09548</em></em>(2018). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 17" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001732" aria-label="citation 1 reference 17">citation 1</option><option value="#auto-BibPLXBIB001738" aria-label="citation 2 reference 17">citation 2</option></select></li> <li id="BibPLXBIB0018" label="[18]" value="18">Bernard&nbsp;E Harcourt. 2014. Risk as a proxy for race: The dangers of risk assessment. <em><em>Fed. Sent'g Rep.</em></em>27(2014), 237. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 18" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00189" aria-label="citation 1 reference 18">citation 1</option></select></li> <li id="BibPLXBIB0019" label="[19]" value="19">George&nbsp;E Higgins, Scott&nbsp;E Wolfe, Margaret Mahoney, and Nelseta&nbsp;M Walters. 2009. Race, Ethnicity, and Experience: Modeling the Public's Perceptions of Justice, Satisfaction, and Attitude Toward the Courts. <em><em>Journal of Ethnicity in Criminal Justice</em></em>7, 4 (2009), 293–310. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 19" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001944" aria-label="citation 1 reference 19">citation 1</option></select></li> <li id="BibPLXBIB0020" label="[20]" value="20">Dan Hurley. 2018. Can an Algorithm Tell When Kids Are in Danger?<a class="link-inline force-break" href="https://www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in-danger.html">https://www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in-danger.html</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 20" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00203" aria-label="citation 1 reference 20">citation 1</option></select></li> <li id="BibPLXBIB0021" label="[21]" value="21">David Jackson and Gary Marx. 2017. Data mining program designed to predict child abuse proves unreliable, DCFS says. <a class="link-inline force-break" href="http://www.chicagotribune.com/news/watchdog/ct-dcfs-eckerd-met-20171206-story.html">http://www.chicagotribune.com/news/watchdog/ct-dcfs-eckerd-met-20171206-story.html</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 21" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002119" aria-label="citation 1 reference 21">citation 1</option></select></li> <li id="BibPLXBIB0022" label="[22]" value="22">Robert Jungk and Norbert Müllert. 1987. <em>Future Workshops: How to create desirable futures</em>. Institute for Social Inventions London. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 22" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002243" aria-label="citation 1 reference 22">citation 1</option></select></li> <li id="BibPLXBIB0023" label="[23]" value="23">Niki Kilbertus, Mateo&nbsp;Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Schölkopf. 2017. Avoiding discrimination through causal reasoning. In <em>Advances in Neural Information Processing Systems</em>. 656–666. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 23" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002349" aria-label="citation 1 reference 23">citation 1</option></select></li> <li id="BibPLXBIB0024" label="[24]" value="24">Christopher Kingsley and Stefania Di&nbsp;Mauro-Nava. 2017. First, do no harm: Ethical Guidelines for Applying Predictive Tools Within Human Services. <em><em>MetroLab Network Report</em></em>(2017). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 24" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002414" aria-label="citation 1 reference 24">citation 1</option></select></li> <li id="BibPLXBIB0025" label="[25]" value="25">Min&nbsp;Kyung Lee. 2018. Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. <em><em>Big Data &amp; Society</em></em>5, 1 (2018), 2053951718756684. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 25" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002526" aria-label="citation 1 reference 25">citation 1</option></select></li> <li id="BibPLXBIB0026" label="[26]" value="26">Min&nbsp;Kyung Lee and Su Baykal. [n. d.]. Algorithmic Mediation in Group Decisions: Fairness Perceptions of Algorithmically Mediated vs. Discussion-Based Social Division. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 26" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002627" aria-label="citation 1 reference 26">citation 1</option></select></li> <li id="BibPLXBIB0027" label="[27]" value="27">Min&nbsp;Kyung Lee, Ji&nbsp;Tae Kim, and Leah Lizarondo. 2017. A Human-Centered Approach to Algorithmic Services: Considerations for Fair and Motivating Smart Community Service Management that Allocates Donations to Non-Profit Organizations. In <em>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</em>. ACM, 3365–3376. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 27" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002728" aria-label="citation 1 reference 27">citation 1</option></select></li> <li id="BibPLXBIB0028" label="[28]" value="28">Christopher&nbsp;T Lowenkamp. 2009. The development of an actuarial risk assessment instrument for US Pretrial Services. <em><em>Fed. Probation</em></em>73(2009), 33. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 28" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00282" aria-label="citation 1 reference 28">citation 1</option></select></li> <li id="BibPLXBIB0029" label="[29]" value="29">John Monahan, Anne Metz, and Brandon&nbsp;L Garrett. 2018. Judicial Appraisals of Risk Assessment in Sentencing. (2018). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 29" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002935" aria-label="citation 1 reference 29">citation 1</option></select></li> <li id="BibPLXBIB0030" label="[30]" value="30">Razieh Nabi and Ilya Shpitser. 2018. Fair inference on outcomes. In <em>Proceedings of the... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence</em>, Vol.&nbsp;2018. NIH Public Access, 1931. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 30" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003050" aria-label="citation 1 reference 30">citation 1</option></select></li> <li id="BibPLXBIB0031" label="[31]" value="31">Jędrzej Niklas, Karolina Sztandar-Sztanderska, and Katarzyna Szymielewicz. 2015. Profiling the unemployed in Poland: social and political implications of algorithmic decision making. <em><em>Fundacja Panoptykon, Warsaw Google Scholar</em></em>(2015). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 31" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00315" aria-label="citation 1 reference 31">citation 1</option></select></li> <li id="BibPLXBIB0032" label="[32]" value="32">Executive&nbsp;Office of&nbsp;the President, Cecilia Munoz, Domestic Policy&nbsp;Council Director, Megan (US Chief Technology Officer Smith&nbsp;(Office of Science, Technology Policy)), DJ&nbsp;(Deputy Chief Technology&nbsp;Officer for Data&nbsp;Policy, Chief Data Scientist Patil&nbsp;(Office of Science, and Technology Policy)). 2016. <em>Big data: A report on algorithmic systems, opportunity, and civil rights</em>. Executive Office of the President. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 32" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003210" aria-label="citation 1 reference 32">citation 1</option></select></li> <li id="BibPLXBIB0033" label="[33]" value="33">Christopher&nbsp;P Parker, Boris&nbsp;B Baltes, and Neil&nbsp;D Christiansen. 1997. Support for affirmative action, justice perceptions, and work attitudes: A study of gender and racial–ethnic group differences.<em><em>Journal of Applied Psychology</em></em>82, 3 (1997), 376. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 33" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003345" aria-label="citation 1 reference 33">citation 1</option></select></li> <li id="BibPLXBIB0034" label="[34]" value="34">Data&nbsp;Futures Partnership. 2017. A Path to Social Licence: Guidelines for Trusted Data Use. <a class="link-inline force-break" href="http://datafutures.co.nz/our-work-2/talking-to-new-zealanders/">http://datafutures.co.nz/our-work-2/talking-to-new-zealanders/</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 34" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003442" aria-label="citation 1 reference 34">citation 1</option></select></li> <li id="BibPLXBIB0035" label="[35]" value="35">Angelisa&nbsp;C Plane, Elissa&nbsp;M Redmiles, Michelle&nbsp;L Mazurek, and Michael&nbsp;Carl Tschantz. 2017. Exploring user perceptions of discrimination in online targeted advertising. In <em>USENIX Security</em>. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 35" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003529" aria-label="citation 1 reference 35">citation 1</option></select></li> <li id="BibPLXBIB0036" label="[36]" value="36">Dillon Reisman, Jason Schultz, K Crawford, and M Whittaker. 2018. Algorithmic impact assessments: A practical framework for public agency accountability. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 36" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003615" aria-label="citation 1 reference 36">citation 1</option></select></li> <li id="BibPLXBIB0037" label="[37]" value="37">O'Brien&nbsp;Kirk Roberts, Yvonne&nbsp;H and Peter&nbsp;J Pecora. 2018. Considerations for Implementing Predictive Analytics in Child Welfare. <em><em>Casey Family Programs</em></em>(2018). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 37" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003716" aria-label="citation 1 reference 37">citation 1</option></select></li> <li id="BibPLXBIB0038" label="[38]" value="38">Douglas Schuler and Aki Namioka. 1993. <em>Participatory design: Principles and practices</em>. CRC Press. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 38" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003840" aria-label="citation 1 reference 38">citation 1</option></select></li> <li id="BibPLXBIB0039" label="[39]" value="39">Nicholas Scurich and John Monahan. 2016. Evidence-based sentencing: Public openness and opposition to using gender, age, and race as risk factors for recidivism.<em><em>Law and Human Behavior</em></em>40, 1 (2016), 36. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 39" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003933" aria-label="citation 1 reference 39">citation 1</option><option value="#auto-BibPLXBIB003937" aria-label="citation 2 reference 39">citation 2</option></select></li> <li id="BibPLXBIB0040" label="[40]" value="40">Hetan Shah. 2018. Algorithmic accountability. <em><em>Phil. Trans. R. Soc. A</em></em>376, 2128 (2018), 20170362. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 40" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB004017" aria-label="citation 1 reference 40">citation 1</option><option value="#auto-BibPLXBIB004018" aria-label="citation 2 reference 40">citation 2</option><option value="#auto-BibPLXBIB004020" aria-label="citation 3 reference 40">citation 3</option></select></li> <li id="BibPLXBIB0041" label="[41]" value="41">Halil Toros and Daniel Flaming. 2018. Prioritizing Homeless Assistance Using Predictive Algorithms: An Evidence-Based Approach. <em><em>Cityscape</em></em>20, 1 (2018), 117–146. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 41" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00414" aria-label="citation 1 reference 41">citation 1</option></select></li> <li id="BibPLXBIB0042" label="[42]" value="42">Michael Veale, Max Van&nbsp;Kleek, and Reuben Binns. 2018. Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making. In <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>. ACM, 440. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 42" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB004236" aria-label="citation 1 reference 42">citation 1</option></select></li> <li id="BibPLXBIB0043" label="[43]" value="43">AJ Wang. 2018. Procedural Justice and Risk-Assessment Algorithms. (2018). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 43" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB004334" aria-label="citation 1 reference 43">citation 1</option><option value="#auto-BibPLXBIB004339" aria-label="citation 2 reference 43">citation 2</option></select></li> <li id="BibPLXBIB0044" label="[44]" value="44">Allison Woodruff, Sarah&nbsp;E Fox, Steven Rousso-Schindler, and Jeffrey Warshaw. 2018. A Qualitative Exploration of Perceptions of Algorithmic Fairness. In <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>. ACM, 656. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 44" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB004430" aria-label="citation 1 reference 44">citation 1</option><option value="#auto-BibPLXBIB004431" aria-label="citation 2 reference 44">citation 2</option></select></li> <li id="BibPLXBIB0045" label="[45]" value="45">Muhammad&nbsp;Bilal Zafar, Isabel Valera, Manuel&nbsp;Gomez Rodriguez, and Krishna&nbsp;P Gummadi. 2016. Fairness Beyond Disparate Treatment &amp; Disparate Impact: Learning Classification without Disparate Mistreatment. <em><em>arXiv preprint arXiv:1610.08452</em></em>(2016). <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 45" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB004547" aria-label="citation 1 reference 45">citation 1</option></select></li> </ul> </section> </section> <section id="foot-001" class="footnote"> <header> <div class="title-info"> <h2><span class="nav-open" id="nav-open" onclick="openNav(this)" tabindex="0" title="navigate to">FOOTNOTE</span></h2> </div> </header> <p id="fn1"> <a href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#foot-fn1"><sup>1</sup></a>Note that the scenarios are all enumerated as “1X”. This enumeration reflects our intention to explore other scenarios in later studies. Those scenarios will be coded as “2X”, “3X” and so forth.</p> <p id="fn2"> <a href="http://delivery.acm.org/10.1145/3310000/3300271/a41-brown.html?ip=169.237.6.227&amp;id=3300271&amp;acc=OA&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E327BD2E78CF22402&amp;__acm__=1575108226_2c198cd880d01f330d94a51502861377#foot-fn2"><sup>2</sup></a>The complete set of derived comfort maps is provided in the supplement.</p> <div class="bibStrip"> <p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from <a href="mailto:permissions@acm.org">permissions@acm.org</a>.</p> <p><em>CHI '19, May 04–09, 2019, Glasgow, Scotland UK</em></p> <p>© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM<br> ACM ISBN 978-1-4503-5970-2/19/05…$15.00.<br>DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3290605.3300271">https://doi.org/10.1145/3290605.3300271</a> </p> </div> </section>   
</body></html>