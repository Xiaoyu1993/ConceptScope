<!DOCTYPE html>
<!-- saved from url=(0250)http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&id=3300273&acc=ACTIVE%20SERVICE&key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <title>Evaluating the Effect of Feedback from Different Computer Vision Processing Stages</title> <!-- Copyright (c) 2010-2015 The MathJax Consortium --> <meta name="viewport" content="width=device-width; initial-scale=1.0;"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <link media="screen, print" rel="stylesheet" href="./paper-chi-algorithm-2_files/bootstrap.min.css"><link media="screen, print" rel="stylesheet" href="./paper-chi-algorithm-2_files/bootstrap-theme.min.css"><link media="screen, print" rel="stylesheet" href="./paper-chi-algorithm-2_files/main.css"><script src="./paper-chi-algorithm-2_files/jquery.min.js" type="text/javascript"></script> <script src="./paper-chi-algorithm-2_files/bootstrap.min.js" type="text/javascript"></script> <script src="./paper-chi-algorithm-2_files/bibCit.js" type="text/javascript"></script> <script src="./paper-chi-algorithm-2_files/divTab.js" type="text/javascript"></script> <script type="text/javascript" src="./paper-chi-algorithm-2_files/MathJax.js"></script> <script type="text/x-mathjax-config;executed=true">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> <style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head> <body id="main" style="margin-left: auto;"><div id="MathJax_Message" style="display: none;"></div><table width="100%"><tbody><tr><td><div class="utilities-area"><div class="logo-section"><div class="show-for-large-up"><a class="navbar-brand" href="http://www.acm.org/"> <img alt="ACM Logo" class="img-responsive" src="./paper-chi-algorithm-2_files/acm_logo.jpg"></a></div><div class="hide-for-large-up"><a class="navbar-brand" href="http://www.acm.org/"> <img alt="ACM Logo" class="img-responsive" src="./paper-chi-algorithm-2_files/acm_logo_mobile.jpg"></a></div></div></div></td></tr><tr height="50px"><td align="left"><button class="ArtNav" id="ArtNav" onclick="openNav(this)" tabindex="0"><span aria-hidden="true">☰</span><span class="ArticleNavi"> Article Navigation</span></button></td></tr></tbody></table> <div id="mySidenav" class="sidenav" aria-hidden="true" tabindex="-1" role="region" aria-labelledby="sidebar_title" style="width: 0px;"><span id="sidebar_title" class="navHead" align="center">Article Navigation</span><a href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#" aria-label="Close article navigation" class="closebtn" onclick="closeNav(true)" tabindex="-1"><span aria-hidden="true">×</span></a><a href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#" class="navAbs" onclick="closeNav(false)" tabindex="-1">Abstract</a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-2"><span style="vertical-align: top;">1</span><span style="display:inline-block;margin-left:5px;width:80%">  INTRODUCTION</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-3"><span style="vertical-align: top;">2</span><span style="display:inline-block;margin-left:5px;width:80%">  PATTERN RECOGNITION IN APPS</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-4"><span style="vertical-align: top;">3</span><span style="display:inline-block;margin-left:5px;width:80%">  RELATED WORK</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-5"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  The importance of system intelligibility</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-6"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Informing users of smart systems</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-9"><span style="vertical-align: top;">4</span><span style="display:inline-block;margin-left:5px;width:80%">  STUDY DESIGN</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-10"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Conditions</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-11"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Procedure</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-16"><span style="vertical-align: top;">5</span><span style="display:inline-block;margin-left:5px;width:80%">  PARTICIPANTS</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-17"><span style="vertical-align: top;">6</span><span style="display:inline-block;margin-left:5px;width:80%">  QUANTITATIVE FINDINGS</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-18"><span style="vertical-align: top;">7</span><span style="display:inline-block;margin-left:5px;width:80%">  QUALITATIVE FINDINGS</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-19"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Participants drew from their existing knowledge</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-20"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Early stage keypoint marker feedback is not easy to understand</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-21"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  When keypoint marker feedback was helpful</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-22"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Split Screen feedback was helpful, but not in the way we expected</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-23"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  When feedback was not helpful</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-24"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Background selection motivation</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-25"><span style="vertical-align: top;">8</span><span style="display:inline-block;margin-left:5px;width:80%">  DISCUSSION</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-26"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Does the processing stage from which feedback is derived impact user understanding?</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-27"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Is keypoint marker feedback intelligible to lay-users?</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-28"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Can keypoint markers mislead if misunderstood?</span></a><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-29"> <span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  Can keypoint markers improve usability and aid users’ interaction?</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-30"><span style="vertical-align: top;">9</span><span style="display:inline-block;margin-left:5px;width:80%">  CONCLUSIONS</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#sec-31"><span style="vertical-align: top;"> </span><span style="display:inline-block;margin-left:5px;width:80%">  ACKNOWLEDGMENTS</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#ref-001"><span style="vertical-align: top;"></span><span style="display:inline-block;margin-left:5px;width:80%">REFERENCES</span></a><hr><a class="navLista" onclick="closeNav(false)" style="display:list-item;" tabindex="-1" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#foot-001"><span style="vertical-align: top;"></span><span style="display:inline-block;margin-left:5px;width:80%">FOOTNOTE</span></a></div><section class="front-matter"> <section> <header class="title-info"> <div class="journal-title"> <h1> <span class="title">Evaluating the Effect of Feedback from Different Computer Vision Processing Stages</span> <br> <span class="subTitle">A Comparative Lab Study</span> </h1> </div> </header> <div class="authorGroup"> <div class="author"> <span class="givenName">Jacob</span> <span class="surName">Kittley-Davies</span>, University Of Southampton, United Kingdom, <a href="mailto:jkd3g11@soton.ac.uk">jkd3g11@soton.ac.uk</a> </div> <div class="author"> <span class="givenName">Rayoung</span> <span class="surName">Yang</span>, UCL Interaction Centre, UK, <a href="mailto:r.yang@ucl.ac.uk">r.yang@ucl.ac.uk</a> </div> <div class="author"> <span class="givenName">Ahmed</span> <span class="surName">Alqaraawi</span>, UCL, UK, <a href="mailto:ahmed.alqaraawi.16@ucl.ac.uk">ahmed.alqaraawi.16@ucl.ac.uk</a> </div> <div class="author"> <span class="givenName">Enrico</span> <span class="surName">Costanza</span>, UCL Interaction Centre, UK, <a href="mailto:e.costanza@ucl.ac.uk">e.costanza@ucl.ac.uk</a> </div> <div class="author"> <span class="givenName">Sebastian</span> <span class="surName">Stein</span>, University of Southampton, UK, <a href="mailto:ss2@ecs.soton.ac.uk">ss2@ecs.soton.ac.uk</a> </div> <div class="author"> <span class="givenName">Alex</span> <span class="surName">Rogers</span>, University of Oxford, UK, <a href="mailto:alex.rogers@cs.ox.ac.uk">alex.rogers@cs.ox.ac.uk</a> </div> </div> <br> <div class="pubInfo"> <p>DOI: <a href="https://doi.org/10.1145/3290605.3300273" target="_blank">https://doi.org/10.1145/3290605.3300273</a> <br>CHI '19: <a href="https://doi.org/10.1145/3290605" target="_blank">Proceedings of CHI Conference on Human Factors in Computing Systems</a>, Glasgow, Scotland UK, May 2019</p> </div> <div class="abstract"> <p><small>Computer vision and pattern recognition are increasingly being employed by smartphone and tablet applications targeted at lay-users. An open design challenge is to make such systems intelligible without requiring users to become technical experts. This paper reports a lab study examining the role of visual feedback. Our findings indicate that the stage of processing from which feedback is derived plays an important role in users’ ability to develop coherent and correct understandings of a system's operation. Participants in our study showed a tendency to misunderstand the meaning being conveyed by the feedback, relating it to processing outcomes and higher level concepts, when in reality the feedback represented low level features. Drawing on the experimental results and the qualitative data collected, we discuss the challenges of designing interactions around pattern matching algorithms.</small></p> </div> <div class="CCSconcepts"> <ccs2012><small> <span style="font-weight:bold;">CCS Concepts:</span> • <strong>Human-centered computing → User interface design</strong>; <em>Human computer interaction (HCI);</em> • <strong>Human-centered computing~User studies;</strong> </small></ccs2012> </div> <div class="classifications"> <div class="author"> <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>controlled study</small>, </span> <span class="keyword"> <small>stop motion animation</small>, </span> <span class="keyword"> <small>computer vision</small>, </span> <span class="keyword"> <small>keypoints</small>, </span> <span class="keyword"> <small>feedback</small>, </span> <span class="keyword"> <small>processing pipelines</small></span> </div> <br> <div class="AcmReferenceFormat"> <p><small> <span style="font-weight:bold;">ACM Reference Format:</span> <br>Jacob Kittley-Davies, Rayoung Yang, Ahmed Alqaraawi, Enrico Costanza, Sebastian Stein, and Alex Rogers. 2019. Evaluating the Effect of Feedback from Different Computer Vision Processing Stages: A Comparative Lab Study. In <em>CHI Conference on Human Factors in Computing Systems Proceedings (CHI 2019), May 4–9, 2019, Glasgow, Scotland UK.</em> ACM, New York, NY, USA 12 Pages. <a href="https://doi.org/10.1145/3290605.3300273" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3290605.3300273</a></small></p> </div> </div> </section> </section> <section class="body"> <section id="sec-2"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">1</span></span> INTRODUCTION</h2> </div> </header> <p>As the power of mobile microprocessors increases and algorithms become ever more efficient, users of smartphones and tablet computers are increasingly being exposed to pattern recognition technologies [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0027" data-original-title="" title="" id="auto-BibPLXBIB00271">27</a>]. While these technologies bring about new opportunities for HCI, they also raise new challenges for interaction designers [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0006" data-original-title="" title="" id="auto-BibPLXBIB00062">6</a>]. One such challenge is how best to convey meaningful feedback so that users can understand the input requirements of these systems and so how best to interact with them. The research community has been quick to respond and there is a growing body of work examining how such systems can be made more intelligible [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0004" data-original-title="" title="" id="auto-BibPLXBIB00043">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0024" data-original-title="" title="" id="auto-BibPLXBIB00244">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0030" data-original-title="" title="" id="auto-BibPLXBIB00305">30</a>]. This paper contributes to this space by examining computer vision as a specific application of pattern recognition. Given the growing number of apps which employ computer vision based pattern recognition (for brevity we refer to them as smart camera apps), this domain is of increasing importance for HCI research.</p> <p>Smart camera apps are commonly designed to simplify user interaction. Amazon's mobile app for example, allows users to search for products using images captured with a device's camera. However, technical limitations (e.g. limited training datasets), environmental challenges (e.g. lighting conditions and shadows), image composition (e.g. noisy backgrounds and camera focus) and unrealistic user expectations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0028" data-original-title="" title="" id="auto-BibPLXBIB00286">28</a>] can all negatively impact user experience - making it difficult for users to reason about unexpected outcomes and in general how best to interact. Therefore, there is a need to support users of smart camera apps in their understanding of system behaviour, so that they can better overcome poor performance and failures. The challenge then is to make the reasons for failures intelligible, without requiring users to become experts in pattern recognition. </p><figure id="fig1"> <img src="./paper-chi-algorithm-2_files/chi2019-43-fig1.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 1:</span> <span class="figure-title">Smart Camera Apps that display keypoint markers feedback to users: left, Amazon and right, Samsung's Bixby.</span> </div> </figure> <p></p> <p>Perhaps it is to address these challenges that a number of commercial smart camera apps include visual feedback, overlaying the camera's viewfinder with visual aids. Two notable examples are the aforementioned Amazon app's search by image feature and Samsung's Bixby, a camera-based search tool<a class="fn" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fn1" id="foot-fn1" data-original-title="" title=""><sup>1</sup></a> (Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig1">1</a>). Both display feedback in the form of keypoint markers - coloured dot visualisations which correspond to features of interest identified by an underlying algorithm. While such visualizations have long been popular as a debugging tool for software developers<a class="fn" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fn2" id="foot-fn2" data-original-title="" title=""><sup>2</sup></a>, to date little is known about their effect on end-user interactions. Their inclusion may simply be motivated by a need to convey background activity, however, their presence raises some interesting questions: (i) are they intelligible to lay users? (ii) do they improve usability and aid users’ interaction around failures? and conversely (iii) can they mislead users if misunderstood?</p> <p>Addressing these questions through a controlled yet ecologically valid study is particularly challenging, because they require observing interaction around failures of the pattern recognition system. Such failures need to be controlled and repeatable, but their causes should not be obvious to study participants’. Moreover, the experimental tasks need to be engaging and enjoyable to motivate participants, have a clear goal and provide discussion points. To address these issues, we present a novel experimental lab study design enabled by a novel smart camera app that we developed. By so doing, we aim to make a methodological contribution to HCI.</p> <p>Leveraging this experimental design and the novel smart camera app, we conducted a between-groups study comparing keypoint markers with no feedback. Twenty participants with no formal technical training took part. Through a combination of quantitative and qualitative methods the results revealed that participants overwhelmingly misinterpreted the meaning of keypoint marker feedback. Participants interpreted them as indicating high level algorithmic explanations (e.g. about recognized objects), while in reality they refer to low-level features of the image (e.g. pixels). To better understand this finding, 20 new participants were exposed to two additional conditions designed around feedback that is actually related to higher level algorithmic explanations. More formally, this second study addresses a fourth question: does the processing stage (lower level vs higher level) from where the feedback is derived impact user understanding? Taken together, the results of the four experimental conditions indicate that keypoint marker feedback derived from the later stages of processing can be an effective means of informing user understanding. In addition, the studies suggest design tensions and implications for designers of pattern recognition feedback.</p> </section> <section id="sec-3"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">2</span></span> PATTERN RECOGNITION IN APPS</h2> </div> </header> <p>The keypoint marker feedback seen in many consumer applications is most likely derived from a keypoint matching algorithm, an intrinsic part of many smart camera apps, e.g. panorama stitching, object detection, gesture recognition and motion tracking. Most keypoint matching algorithms involve three stages of processing: (i) identify distinctive points of interest in an image (the keypoints), (ii) programmatically describe them, so that the description is resilient to geometric variations e.g. rotation, scale and perspective, and photometric variants e.g. contrast and brightness, and (iii) compare the descriptions with those of another image. How the results of this comparison process are used is application specific. In panorama stitching for example, the closest matching descriptions between images are assumed to represent the same point in the physical world. Using their relative changes in position the images can be transformed such that the keypoints overlap creating a new combined image with a wider field of view.</p> </section> <section id="sec-4"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">3</span></span> RELATED WORK</h2> </div> </header> <section id="sec-5"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> The importance of system intelligibility</h3> </div> </header> <p>The significance of how users understand system operation has received considerable attention from the fields of HCI and psychology. Much of the early work in this space centered around the theoretical construct of Mental Models - users’ internal representations which allow them to explain and predict the actions of a system, permitting them to reason about their interactions before committing to an action [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0019" data-original-title="" title="" id="auto-BibPLXBIB00197">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0020" data-original-title="" title="" id="auto-BibPLXBIB00208">20</a>]. This work indicated that the accuracy of a user's mental model could dramatically impact their capacity to interact effectively with a system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0003" data-original-title="" title="" id="auto-BibPLXBIB00039">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0007" data-original-title="" title="" id="auto-BibPLXBIB000710">7</a>] and that users with coherent mental models perform tasks more efficiently [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0011" data-original-title="" title="" id="auto-BibPLXBIB001111">11</a>]. More recently, studies have shown that users with more complete mental models are more likely to make systems operate to their satisfaction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0005" data-original-title="" title="" id="auto-BibPLXBIB000512">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0016" data-original-title="" title="" id="auto-BibPLXBIB001613">16</a>] and that flawed mental models can result in confusion, misconceptions, dissatisfaction and erroneous interactions [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0015" data-original-title="" title="" id="auto-BibPLXBIB001514">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0026" data-original-title="" title="" id="auto-BibPLXBIB002615">26</a>]. When specifically considering systems which employ pattern recognition or machine learning technologies, the overestimation of a system's intelligence or capabilities has been shown to negatively impact user interaction [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0001" data-original-title="" title="" id="auto-BibPLXBIB000116">1</a>]. This can lead to users becoming over-reliant and so less vigilant to system failures [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0029" data-original-title="" title="" id="auto-BibPLXBIB002917">29</a>]. It can also result in unrealistic expectations [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0028" data-original-title="" title="" id="auto-BibPLXBIB002818">28</a>], the violation of which can impact user trust [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0012" data-original-title="" title="" id="auto-BibPLXBIB001219">12</a>].</p> <p>User understanding can be affected by many factors, from simple misunderstandings of terminology [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0002" data-original-title="" title="" id="auto-BibPLXBIB000220">2</a>] to complex subconscious biases. A recent study of floor cleaning robots [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0009" data-original-title="" title="" id="auto-BibPLXBIB000921">9</a>] describes how users rated a cleaning robot's performance to be better when they had witnessing it in motion. The evidence supporting the need to inform users of underlying processes is compelling. However, the means by which this can be achieved is non-trivial when discussing smart systems, with the inherent complexity of these systems potentially inhibiting interaction design [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0006" data-original-title="" title="" id="auto-BibPLXBIB000622">6</a>]. Our work contributes to this space, investigating the importance of user understanding on effective interaction with pattern recognition and computer vision systems. Further to this, we report observations of misconceptions and the consequences for user interaction.</p> </section> <section id="sec-6"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Informing users of smart systems</h3> </div> </header> <p>Investigations of a context aware system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0017" data-original-title="" title="" id="auto-BibPLXBIB001723">17</a>], which employs machine learning algorithms to make decisions, have demonstrated not only the benefits of making the motivations behind automated decisions salient to novice users (via text notifications), but that explanations of why system behaviour occurred result in better user understanding than explanations of why not, an observation supported in a later study [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0013" data-original-title="" title="" id="auto-BibPLXBIB001324">13</a>] of simulated driver assistants (albeit through a different modality). Researchers found that audible messages which report only the intended actions of the system had little impact on driver safety, whereas messages which conveyed why these actions were necessary had a positive influence. In this paper we expand on this work, examining how effective existing feedback visualisations are at informing user understanding and discuss the implications for designing effective visual feedback for such systems.</p> <section id="sec-7"> <p><em> Interaction with pattern matching systems.</em> Software platforms such as Crayons [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0008" data-original-title="" title="" id="auto-BibPLXBIB000825">8</a>] and Eyepatch [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0018" data-original-title="" title="" id="auto-BibPLXBIB001826">18</a>] were specifically developed to insulate users from the complexities of computer vision and pattern matching technologies. They theorize that by providing users with interfaces that facilitate rapid trial-and-error [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0008" data-original-title="" title="" id="auto-BibPLXBIB000827">8</a>], the most effective solutions to classification problems can be found. However, as pattern recognition technologies become increasingly complex, Patel, et al [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0022" data-original-title="" title="" id="auto-BibPLXBIB002228">22</a>] have suggested that successful implementation can only be achieved with a deeper understanding of the inner-workings of the processes. In contrast to Crayons and Eyepatch, DejaVu [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0010" data-original-title="" title="" id="auto-BibPLXBIB001029">10</a>] was developed to expose domain-expert programmers to computer vision technologies, with the ambition of aiding code debugging. The system allows images passing through the various stages of processing to be inspected and an interactive timeline interface lets users record and examine data flow temporally. Although a small user study was conducted, the focus was system functionality rather than the user experience. Our work builds on this by conducting a lab study specifically designed to evaluate the impact of exposing lay users to pattern matching processes.</p> <p>Zhao et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0031" data-original-title="" title="" id="auto-BibPLXBIB003130">31</a>] conducted a study examining lay-users’ interactions with an augmented reality pattern recognition system designed to assist users with low vision in a product search task, by highlighting regions of a head mounted display with visual feedback. The feedback in this case was derived from the output of the pattern matching processing pipeline. Our work builds on the work of Zhao et al, exploring the relationship between the origin of the data which informs feedback and users’ interpretation of it. </p><figure id="fig2"> <img src="./paper-chi-algorithm-2_files/chi2019-43-fig2.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 2:</span> <span class="figure-title">Creating an animation - (1) Set up the background scene with the character in its starting position. (2) Capture an image which encapsulates the scene and the character. (3) Reposition the character. (4) Capture another frame. (5) Preview the captured frames as an animation. Delete and retake if not acceptable, or repeat stages 3 to 5 until the animation is complete.</span> </div> </figure> <p></p> </section> <section id="sec-8"> <p><em> Exposing data processing.</em> Exposing the underlying data processing is an idea which has been explored in the domain of machine-learning. The creators of Gestalt [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0021" data-original-title="" title="" id="auto-BibPLXBIB002131">21</a>] an integrated development environment (IDE) designed specifically to assist programmers creating software which makes use of machine learning technologies, demonstrated through lab studies, that exposing data at various stages of a process significantly improves programmers’ ability to identify and correct errors in their code. Similarly, Prospector [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0014" data-original-title="" title="" id="auto-BibPLXBIB001432">14</a>] which facilitates the probing of the predictive models by data scientists, was shown to help them understand how features affect the overall predictions. They report that by allowing users to adjust input variables and see through visualisations to how the model responds, users gained deeper insights into how the model worked. Our work builds on these findings by demonstrating the capacity of algorithmic feedback to support lay-user understanding, but also how it can lead to misconceptions if designed badly. We expose a number of design implications and discuss the challenges of designing effective visual feedback for such systems.</p> </section> </section> </section> <section id="sec-9"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">4</span></span> STUDY DESIGN</h2> </div> </header> <p>We designed and conducted a between-groups study with four conditions. The conditions were selected to examine the impact of common visual feedback techniques and assess if the processing pipeline stage from which the feedback is derived has an effect on user understanding. Developing an ecologically valid and testable experimental task which incorporated a keypoint matching algorithm proved non-trivial. The task needed to provide sustained exposure to algorithmic feedback so that participants could observe and reason about the feedback. Further to this, participants must experience instances of failure and success. The task therefore should be controllable, but in a way not obvious to participants. In addition, it would be advantageous for the task to be enjoyable to motivate interaction, have a clear goal and provide discussion points. Through experimentation a task which best satisfied these criteria was developed, the creation of stop-motion animations.</p> <p>To create a stop-motion animation, an animator must capture a series of still images (frames) of a given scene. By incrementally moving artifacts (characters) between frames the illusion of animation can be achieved i.e. when the frames are played back in order the characters appear to move autonomously in relation to the static elements of the scene (e.g. the background). Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig2">2</a> demonstrates the process. Traditionally stop motion animations are created using cameras where the position and angle are strictly controlled e.g. held in a tripod. To incorporate pattern recognition technologies in to our study design we replaced the controlled camera with a handheld tablet computer and bespoke app (Anim8<a class="fn" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fn3" id="foot-fn3" data-original-title="" title=""><sup>3</sup></a>) which employs a keypoint matching algorithm<a class="fn" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fn4" id="foot-fn4" data-original-title="" title=""><sup>4</sup></a> to align each frame to its predecessor - a process of stabilization. This process makes all frames appear to have been captured from the same physical location even though the camera's position and angle vary. The keypoints with the closest descriptions are matched and assumed to point to the same physical feature in both frames. The most recently captured image can then be transformed so that its keypoints overlap its predecessors. Characters which have been moved between frames will create erroneous mappings, however if enough matches are found for the elements of the scene which have remained static (e.g. the background) then the matches associated with the moving characters will be treated as outliers and ignored. </p><figure id="fig3"> <img src="./paper-chi-algorithm-2_files/chi2019-43-fig3.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 3:</span> <span class="figure-title">When too few matching keypoints are identified in the background, the stabilization process can result in an image transformed such that the character appears to remain stationary and the background becomes distorted.</span> </div> </figure> <figure id="fig4"> <img src="./paper-chi-algorithm-2_files/chi2019-43-fig4.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 4:</span> <span class="figure-title">Examples of the feedback conditions presented by the Anim8 application and their relationship to the processing pipeline (a, b, c, d). Also the preview interface (e). Note: To see these images animate see supplemental materials.</span> </div> </figure> <p></p> <p>In order for the stabilization process to work effectively it is critical that the static elements of the scene are feature rich, i.e. the algorithm can identify many keypoints. If there are too few then the transformation process may output an image where the background is distorted and the character remains stationary (Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig3">3</a>). Leveraging this limitation, the likelihood of whether the stabilization process will succeed or fail can be controlled - by providing feature rich and feature poor backgrounds participants of the study can be exposed to situations where the stabilization process succeeds and fails respectively. Factors such as lighting conditions, shadows and camera angle make this form of manipulation not immediately obvious to study participants.</p> <p>Through pilot studies we concluded that four animation tasks with 4 to 5 frames per task provides sufficient exposure. We designed the tasks to assess whether feedback derived from the stabilization process can help participants develop better understandings of the systems’ needs. To create discussion points and elicit user understanding we ask participants to choose one of three background options in the last two animation tasks (3 options per task). The feature richness of the three background options varied and thus the likelihood of the stabilization process succeeding varied (Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig6">6</a>).</p> <section id="sec-10"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Conditions</h3> </div> </header> <p>To explain the study conditions, we describe them in relation to the computer vision pipeline employed by Anim8 (Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig4">4</a>). It should be noted that we did not explain the feedback nor point out its presence to participants. This was done to mirror the experiences of current consumer smart camera app users.</p> <p><em>No-Feedback (Figure&nbsp;4a)</em>. This condition was included as a baseline. The input images to the pipeline were presented back to participants without any additional feedback.</p> <p><em>Keypoints (Figure&nbsp;4b)</em>. The camera's viewfinder was augmented with keypoint markers which indicate the locations at which keypoints had been detected in stage 2. It is important to note that not all the identified keypoints will be matched. Matches where the descriptions are considered too dissimilar are deemed outliers and are ignored by the stabilization process. Despite this, the location, distribution and volume of identified keypoints are good indicators for the potential success of the stabilization process.</p> <p><em>Matching-Keypoints (Figure&nbsp;4c)</em>. Again the viewfinder was augmented with keypoint markers, however in this case only those which have been successfully paired with keypoints in the previous frame were displayed (Stage 4).</p> <p><em>Split-Screen (Figure&nbsp;4d)</em>. This condition represents the final stage of processing. The viewfinder was divided into two equal halves. On the left: the input image updated in real-time (as per No-Feedback condition). On the right: the image outputted by the processing pipeline (update every ~120ms).</p> <p>The No-Feedback and Keypoints conditions were compared first, while the Matching-Keypoints and Split-Screen conditions were included at a later stage, as described in the Introduction.</p> </section> <section id="sec-11"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Procedure</h3> </div> </header> <p>All studies were conducted in the same empty windowless meeting room (so lighting conditions could be controlled) on a university campus. Two experimenters were present at all times - one to conduct the experiment and the other to observe, take notes and make audio recordings.</p> <p>At the start of the study participants received written instructions detailing: (i) the procedure necessary to create stop-motion animations, (ii) how Anim8 uses computer vision technologies to remove the need for a tripod, and (iii) a high level explanation of the image processing operations - that Anim8 tries to align images by looking for things in each image which are not supposed to have moved, for example the background. After reading the instructions participants were asked to stand up while performing the animation tasks.</p> <p>Participants were tasked with creating 4 stop motion animations. Animating a two dimensional cardboard character (approximately 8cm by 5cm in size) moving across an A3 printed background (see Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig5">5</a> for examples). To ensure that all participants had a good understanding of how to use the Anim8 application, the experimenter demonstrated the capture, playback and delete operations prior to the first task commencing. Whilst demonstrating the capturing of a frame, the participants were advised to ensure the printed background scene was fully encapsulated in the camera's viewfinder and that the desk should not be visible. This was done to prevent features other than those in the scene impacting the outcome of the experiment (this was not explained to the participant). The participants were also advised that if they needed any assistance regarding the operation of the application during the study, then they could ask at any time.</p> <p>Prior to each animation task, the experimenter provided each participant with the necessary materials (i.e. a character to animate and static background scene / scenes) and an instruction sheet detailing an example path for the character to follow, along with the number of frames expected (4 to 5). On completion of the task, the participant was asked to play back the animation they had created to the experimenter. The tasks were conducted in the same order for all participants to ensure that they experienced both successful and unsuccessful attempts. The tasks were structured as follows: </p><figure id="fig5"> <img src="./paper-chi-algorithm-2_files/chi2019-43-fig5.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 5:</span> <span class="figure-title">Example frame for each of the animation tasks.</span> </div> </figure> <p></p> <section id="sec-12"> <p><em> Task 1.</em> was designed to allow participants to familiarise themselves with the UI and reassure them that the app works as described. To this end, a feature rich background (Figure&nbsp;5a) which proved in testing to work with almost no failures was provided, making the task easy to succeed. On completion, the experimenter asked how the participants found using the app and if they had any queries.</p> </section> <section id="sec-13"> <p><em> Task 2.</em> was designed to highlight the limitations of the system. The background in this task (Figure&nbsp;5b) proved in testing to always fail. As it was impossible to complete this task, the experimenter would intervene after a time limit of 2 minutes, if the participant had not already raised concerns. The experimenter would ask the participants to explain what was happening and if they knew why it did not work, before suggesting that they proceed to the next task for brevity.</p> </section> <section id="sec-14"> <p><em> Task 3.</em> was designed to assess users’ understanding and create a point of discussion in the interview. Participants were asked to choose the background they felt would work best for the app from a selection of 3 backgrounds (see Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig6">6</a>). Participants were advised that they could preview them through the application's viewfinder if they wished. The backgrounds offered had previously been assessed and ranked according to the algorithm's ability to effectively identify features within them. One of the backgrounds consistently failed in testing and the remaining two consistently worked well, although one was more visibly feature rich than the other. The motivation for presenting users with this range of background options was to make the different levels of detail between the backgrounds less obvious. Once the participant completed this animation task, they were asked why they had selected that specific background.</p> </section> <section id="sec-15"> <p><em> Task 4.</em> followed the same structure as Task 3, with a new character and set of 3 backgrounds (see Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig6">6</a>). This last task was designed to sustain participant interaction with the application, collect an additional data point and further assess user understanding i.e. what, if anything, had been learned in Task 3. </p><figure id="fig6"> <img src="./paper-chi-algorithm-2_files/chi2019-43-fig6.jpg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 6:</span> <span class="figure-title">Background options presented to participants in Tasks 3 (Top row) and Task 4 (Bottom row). Left: Likely to fail, Center and Right: Likely to succeed.</span> </div> </figure> <p></p> <p>At the end of the study a semi-structured interview was conducted. The interview began by asking participants if their experience in Task 3 and Task 4 had given them a better understanding of why the animation in Task 2 resulted in failure. Using this as a starting point, the experimenter asked further questions to assess the participants’ understanding of the algorithm and their motivations for selecting the backgrounds in Task 3 and Task 4. For the participants of conditions where feedback was presented in the viewfinder, the experimenter also asked what they thought it represented and if they used it in their decision making.</p> </section> </section> </section> <section id="sec-16"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">5</span></span> PARTICIPANTS</h2> </div> </header> <p>We recruited 40 participants (15F, 25M) from the university participant pool which includes university staff, students and the general public. Anyone who expressed interest was allowed to participate in the study, so long as they did not identify as having technical hobbies or interests (e.g. computer programming), were not in technical employment (e.g. lab assistant) and were not technically educated (e.g. no degree in computing or engineering related subjects). Participants were also required to have normal or corrected to normal vision. Each participant received a £10 payment for their participation. Of the 40 participants 29 reported to be in education and 11 in full time employment. Participants’ backgrounds were diverse with the most common being Business &amp; Economics (13) followed by Social Sciences (9) Law (5), Languages (5), Art (4), Accountancy (2), Medicine (1) and Geography (1). One participant was aged between 40 and 49 years, 6 between 30-39 and 33 between 20-29. For more detailed information please see the supplemental materials.</p> <p>Ten participants were randomly assigned to each condition. For conciseness, we will refer to participants by condition and subject number, for example, K7 was subject number 7 of the Keypoints condition. Prefixes N, M and S refer to the No-Feedback, Matching-Keypoint and Split Screen conditions respectively.</p> <div class="table-responsive" id="tab1"> <div class="table-caption"> <span class="table-number">Table 1:</span> <span class="table-title">No. Participants who selected a “correct background” i.e. suited to the needs of the app.</span> </div> <table class="table"> <thead> <tr> <th style="text-align:left;"> </th><th> </th><th> </th></tr> <tr> <th style="text-align:left;"> </th><th style="text-align:center;">Task 3</th> <th style="text-align:center;">Task 4</th> </tr> </thead> <tbody> <tr> <td style="text-align:left;">No-Feedback</td> <td style="text-align:center;">10</td> <td style="text-align:center;">10</td> </tr> <tr> <td style="text-align:left;">Keypoints</td> <td style="text-align:center;">7</td> <td style="text-align:center;">10</td> </tr> <tr> <td style="text-align:left;">Matching-Keypoints</td> <td style="text-align:center;">10</td> <td style="text-align:center;">10</td> </tr> <tr> <td style="text-align:left;">Split-Screen</td> <td style="text-align:center;">9</td> <td style="text-align:center;">10</td> </tr> </tbody> </table> </div> </section> <section id="sec-17"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">6</span></span> QUANTITATIVE FINDINGS</h2> </div> </header> <p>To quantitatively assess the effect of feedback across the conditions, three researchers independently coded participants’ responses to questions pertaining to their background selections (taken from researcher notes and transcripts of audio recordings). This coding process was specifically focussed on the participants’ understanding of how the system works (in contrast, in the next section we report a further analysis of the data through broader, more general coding). In particular, a participant's response was coded as correct understanding if they described how the presence of distinctive shapes and features in the background positively impacted the app's ability to align frames. For example, the following statements were coded as demonstrating a correct understanding: I think it picks up the shapes on the picture and it [...] then compares the position of the dots on the other one [...] the next picture? So it can tilt the frame accordingly (K9) or because the background is distinct enough (N6). If a participant reported motives not connected to the requirements of the app or their understanding of what is significant was incorrect they were coded as incorrect understanding. For example, the following statements were coded as demonstrating an incorrect understanding: Because it's nice and colourful (N8) or [...]it looked more homogenous than the other ones. So I thought [...] it would be easier to take the photos like this (K2).</p> <p>Table&nbsp;<a class="tbl" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#tab1">1</a> summarizes the background selections made by participants in Task 3 and Task 4 and Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig7">7</a> shows whether their selection was based on a correct understanding of the stabilization processes.</p> <p>To compare participants’ understanding between the conditions we consider the total number of answers which demonstrated a correct understanding in Task 3 and Task 4 (Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig7">7</a>). For example, 7 of the 10 participants in the Split-Screen condition demonstrated a correct understanding in Task 3 and 9 participants in Task 4, giving a summed value of 16. A chi-square test of the summed values revealed a statistically significant difference (chi-square=8.33, p=.040, df=3, Cramer's V=0.323). To better understand the differences between the conditions, we analysed the chi-squared standardized residuals (presented in Table&nbsp;<a class="tbl" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#tab2">2</a>). It can be noticed that the standardized residuals are larger (in absolute value) for the Keypoints and Split-Screen conditions, suggesting that these two conditions explain the significance of the chi-square test. A chi-square test also shows no statistically significant differences for correct background selections (chi-square=6.316,p=.097,df=3), nor when testing the tasks individually<a class="fn" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fn5" id="foot-fn5" data-original-title="" title=""><sup>5</sup></a>. It should be noted that participants sometimes selected a ’feature-rich’ background for aesthetic reasons rather than because it would make the app work better (as instructed), failing to demonstrate correct understanding. In the next section we discuss our qualitative findings and the role of background selection further. </p><figure id="fig7"> <img src="./paper-chi-algorithm-2_files/chi2019-43-fig7.svg" class="img-responsive" alt="" longdesc=""> <div class="figure-caption"> <span class="figure-number">Figure 7:</span> <span class="figure-title">No. Participant responses coded as “correct understanding” when reporting their motivation for background selection in task 3 and task 4.</span> </div> </figure> <p></p> <div class="table-responsive" id="tab2"> <div class="table-caption"> <span class="table-number">Table 2:</span> <span class="table-title">Standard residual results of the No. participants who demonstrated a “correct understanding”.</span> </div> <table class="table"> <thead> <tr> <th style="text-align:left;"> </th><th> </th><th> </th><th> </th></tr> <tr> <th style="text-align:left;"> </th><th style="text-align:center;">Count</th> <th style="text-align:center;">Expected</th> <th style="text-align:center;">Std Residual</th> </tr> </thead> <tbody> <tr> <td style="text-align:left;">No-Feedback</td> <td style="text-align:center;">10</td> <td style="text-align:center;">12</td> <td style="text-align:center;">-0.6</td> </tr> <tr> <td style="text-align:left;">Keypoints</td> <td style="text-align:center;">8</td> <td style="text-align:center;">12</td> <td style="text-align:center;">-1.2</td> </tr> <tr> <td style="text-align:left;">Matching-Keypoints</td> <td style="text-align:center;">14</td> <td style="text-align:center;">12</td> <td style="text-align:center;">0.6</td> </tr> <tr> <td style="text-align:left;">Split-Screen</td> <td style="text-align:center;">16</td> <td style="text-align:center;">12</td> <td style="text-align:center;">1.2</td> </tr> </tbody> </table> </div> </section> <section id="sec-18"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">7</span></span> QUALITATIVE FINDINGS</h2> </div> </header> <p>Transcripts of all audio recordings and researchers’ notes collected during the studies were also independently coded by three researchers in a second round of analysis. Codes were initially drawn from research questions and then supplemented with those that emerged from the interviews before being grouped by consensus. In the subsequent subsections we detail these groups and give example quotations. First however, we would like to note that overwhelmingly participants reported the task to be interesting and entertaining. This suggests that the experimental task was sufficiently engaging and participants were invested in creating animations successfully.</p> <section id="sec-19"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Participants drew from their existing knowledge</h3> </div> </header> <p>First we note, that when asked about previous experience with computer vision applications, participants mentioned QR Code scanning, Facebook and Instagram (none of which provide visual feedback). No participants reported using Amazon or Bixby's search by image, or any other application which provides keypoint feedback.</p> <p>In the No-Feedback condition, half of the participants demonstrated a correct understanding. These participants explained that having elements in the background which were more detailed (N1), most defined (N7), distinct (N6) or prominent (N2) would help the app because they were good reference points for alignment. The remaining five participants had an incorrect understanding and in the main focussed on the aesthetics, e.g I thought the clouds would go really well with [...] the hot air balloon (N9).</p> <p>Interestingly, participants in the No-Feedback condition selected a correct background more often than participants in the Keypoints condition (Table&nbsp;<a class="tbl" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#tab1">1</a>). Participants K2, K4 and K8 of the Keypoints condition made associations between the keypoint markers and their experience of other applications, suggesting that the keypoint markers functioned in much the same way as the autofocus on digital cameras, in that they highlight regions on which the camera is focussing. Whether these analogies are helpful is not clear. One of the participants who drew such parallels made good choices when selecting backgrounds, while the remaining two were misled by their assumptions - K2 for example, chose a feature poor background for Task 3, expecting that a plain background would make it easier for the app to identify the character.</p> </section> <section id="sec-20"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Early stage keypoint marker feedback is not easy to understand</h3> </div> </header> <p>Participants of the Keypoints condition broadly failed to understand the meaning of keypoint markers and how it related to low-level features of interest to the algorithm (30% demonstrated a correct understanding in Task 3 and 50% Task 4). Participants K1, K2 and K3 incorrectly thought that the keypoint markers were highlighting regions where the algorithm had identified a moving object, something the user intended to animate. These participants theorised that if the algorithm succeeds in finding the objects which are meant to move, then the algorithm will be able to successfully transform the captured images to create animations e.g. K2 said these dots might help show that the focus of the photo is the [character] [...] if I have these dots around the [character] then the image will be clearer. K2 and K3 both selected the worst background option for Task 3. They justified their choice by saying that among the three options the plainest background would work best because it would make the identification of the character easier for the algorithm e.g. K3, when asked why they chose a plain background in Task 3, said it was because [the app] could be confused about the subject of the picture. Both K2 and K3 expressed confusion when keypoint markers appeared in locations which did not fit their understanding of how the system works i.e. on the background instead of the character. K2 remarking: [keypoint markers] try to capture the [character] in the photo, a balloon, [...], but it's not on the balloon and K3, [if keypoint markers] mean the [character] is moving, [...] I don't understand why [keypoint] markers are showing up on the cloud, not the [character]. Despite witnessing evidence to the contrary both participants failed to correct their misunderstanding, a behaviour pattern previously reported in work on intelligent system [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0026" data-original-title="" title="" id="auto-BibPLXBIB002633">26</a>].</p> </section> <section id="sec-21"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> When keypoint marker feedback was helpful</h3> </div> </header> <p>The quantity of the keypoint markers was the most commonly reported explanation of how participants took into account Keypoint feedback. For example, K1 explained that if [...] in background, [I] see a lot of dots. I can tell that background is definite. When I did the [animation of the] plane [for which the app failed], there were only 1 or 2 dots. K6 stated that if there is nothing [in the background], it's not going to work. [If] something is there it's going to work. However, only four participants demonstrated a better understanding which was consistent with the workings of the stabilization process. These participants noticed how and where the keypoint markers appeared and were able to develop more specific theories of how the algorithm identifies keypoint markers within an image. For example, K10 correctly speculated that the algorithm pick[s] up the shape and areas of heavy contrast.</p> <p>In the Matching-Keypoint condition, six of the ten participants reported the feedback to be helpful. Of these participants, three described the keypoint markers as indicators, reporting what the algorithm was doing: I can see what the dots are surrounding. [...] I know what it's doing (M10), when I saw [keypoints markers], it was more reassuring [...] saying you're doing it right (M7), and the app is trying to match between images [...] things which the app sees in this image which it also saw in the previous image (M1). The other three participants explained that they saw the keypoint markers as guides, that the keypoint markers were designed to help them test if the background image would work or not: the dots showed if the picture would work out (M6), I can tell what's the problem of the image (M8) and [the keypoints] might help you pick a background (M5).</p> <p>Participants in the Keypoints condition tended to overestimate the meaning of the Keypoint feedback and relate the meaning to higher level concepts, such as the separation of background and foreground objects. In this regard Matching-Keypoints appeared to be more intuitive as its meaning is more inline with user expectation. M1 for example, reported that when the app didn't work in Task 2 he did not know why. During Task 3, he speculated that the colour might have an effect (lighter or darker colour), but found through experimentation that this was not the case. He then correctly theorized that the app needed distinct features. He explained, The dots meant like it's picking distinct points throughout the image. [...] I think [the app is] re-mapping the points that [it had] taken in an image before. I think that's what it's trying to do.</p> </section> <section id="sec-22"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Split Screen feedback was helpful, but not in the way we expected</h3> </div> </header> <p>Seven participants in the Split-Screen condition also reported the feedback to be helpful. Four participants suggested that it acted as a cue, indicating when best to capture a frame e.g. The preview helped me decide when to take a picture (S7) or I [wait] for the preview to stabilize before taking the picture (S3). An artifact of the stabilization processes implementation is a flickering effect which occurs when the system is rapidly toggling between a successful transform and a failure. This strictly speaking is a usability bug which participants reappropriated, using it as a means of gauging the likelihood of a successful transform e.g. If it was flickering I wouldn't take the picture (S7), and I waited for a clear picture [...] then hit capture (S4).</p> <p>Another unexpected way of using Split-Screen feedback was described by two participants (S7 and S2). They used the feedback to position the camera in the same place as the previous image, S7 commenting the preview tells me what angle to take the picture from. Both participants would keep moving the camera until the left and right images matched in the preview i.e. the alignment transformation was minimal. This approach does in fact help make better quality animations, however it is not how the app was intended to be used and this process of positioning was very time consuming for the participants.</p> </section> <section id="sec-23"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> When feedback was not helpful</h3> </div> </header> <p>Five participants in the Split-Screen condition and three in the Matching-Keypoints condition reported the feedback to be distracting or unhelpful. For example, I found the split screen very distracting and would rather not see it (S4), I found the dots distracting because it ruined the focus at times (M4), They were a bit annoying, they get in the way (M1) and they could be obstructive (M6). Interestingly, S6 described the feedback as unhelpful because they prefered to frame the photo from memory, using the viewfinder to align the camera with features they had identified in the background. To this end the preview was unhelpful because the split screen design reduced the size of the viewfinder. These comments illustrate the risk that feedback visualisations can be distracting.</p> </section> <section id="sec-24"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Background selection motivation</h3> </div> </header> <p>Although all participants selected a correct background in Task 4, not all provided a correct explanation. Participants responses when asked why they chose the background image they selected in Task 3 and Task 4 were coded into one of two categories: aesthetic - they were motivated by how the image looked, and detail - where they stated in some way that the level of detail was important (including incorrect understandings). Aesthetics was the primary motivation for 27 selections out of 80 (10 No-Feedback, 9 Keypoints, 5 Matching-Keypoints and 3 Split-Screen), with detail accounting for the remaining 53 selections (10 No Feedback, 11 Keypoints, 15 Matching-Keypoints and 17 Split-Screen). It should be noted that it is by chance that some of our participants considered the correct background to be more aesthetically pleasing.</p> </section> </section> <section id="sec-25"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">8</span></span> DISCUSSION</h2> </div> </header> <p>In the introduction we set out a series of questions. In this section we discuss the outcomes of our study using these questions as a scaffold.</p> <section id="sec-26"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Does the processing stage from which feedback is derived impact user understanding?</h3> </div> </header> <p>Our results indicate that feedback derived from the later stages of the processing pipeline (Matching-Keypoints and Split-Screen) are more effective at informing users’ understanding. The chi-square test of user understanding reveals a significant difference between conditions, with the standard residuals indicating the Keypoints and Split-Screen are responsible. More participants of the Split-Screen condition demonstrated a correct understanding of how the system works than participants of any other condition (Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig7">7</a>), with Matching-Keypoints second. In contrast, participants in the Keypoints condition performed worse than participants who received no feedback at all.</p> <p>Despite users understandings varying between conditions, most participants across all conditions were successful in selecting a correct background (see Figure&nbsp;<a class="tbl" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#tab1">1</a>). As mentioned above, participants sometimes selected the correct background for aesthetic reasons, rather than to make the algorithm work (as requested by the study instructions). As a consequence, instead of using selection as a measure of understanding, we rely only on the participants’ explanations of <em>why</em> they selected a specific background.</p> </section> <section id="sec-27"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Is keypoint marker feedback intelligible to lay-users?</h3> </div> </header> <p>More participants in the Matching-Keypoints condition were able to correctly describe the input requirements of the system in comparison with those who received no additional information in the form of feedback (No-Feedback). Interview responses indicate that users have a tendency to interpret feedback as an outcome rather than a progress notification of an intermediary stage. In this regard Matching-Keypoints appeared to be more intuitive, as their meaning is more inline with user expectation. We tentatively propose that keypoint markers can be used to inform user understanding, so long as the meaning being conveyed is inline with user expectations.</p> </section> <section id="sec-28"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Can keypoint markers mislead if misunderstood?</h3> </div> </header> <p>Given that the Keypoints and Matching-Keypoints conditions utilise exactly the same feedback visualisation (keypoint markers), the result showing that Keypoints condition participants were least able to understand the needs of the algorithm (Figure&nbsp;<a class="fig" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#fig7">7</a>) suggests that they may have been detrimental to user understanding. While the keypoint markers are a good indicator of the future stabilization processes success, participants commonly understood them to represent the final output, that they represented regions where the stabilization process had identified matches. It is feasible that this misconception could result in users using the markers in ways which inhibit their interactions. Indeed, Keypoints condition participants’ interview responses indicate a disconnect between their interpretation of feedback and the actual information conveyed e.g. K3, [if keypoints] mean the [character] is moving, [...] I don't understand why keypoints are showing up on the cloud, not the [character].</p> </section> <section id="sec-29"> <header> <div class="title-info"> <h3> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> Can keypoint markers improve usability and aid users’ interaction?</h3> </div> </header> <p>The inherently visual nature of computer vision processes, both in their input and also the intermediate stages, makes visual feedback the logical medium through which to deliver feedback [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0010" data-original-title="" title="" id="auto-BibPLXBIB001034">10</a>]. However, participants in our studies, at times reported the feedback to be distracting or obtrusive (e.g. M1 They were a bit annoying, they get in the way). This highlights a design tension between attracting attention and causing distraction, and between being informative and not overwhelming. These tensions are well understood in graphic design, particularly around the design of interactive visualizations. However, the situation here is more complex. Some aspects of algorithm design are conceptually simple and naturally map to visual representations. Keypoints for example, are a concept that lend themselves to being represented pictorially e.g. by marking their physical location with geometric points. It could at first be tempting to see this as an example of form follows function [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0025" data-original-title="" title="" id="auto-BibPLXBIB002535">25</a>], however when dealing with the design of feedback for systems which employ pattern matching algorithms, we argue that the form follows function principle requires careful interpretation. What is function in this case? At first, it may seem to be the technical function of the algorithm, but this is not the case. We need to remind ourselves that the function is instead the function to help users understand what the system does. One implication then, is that to design feedback, it may be beneficial to distance oneself from the question of how algorithmic steps and internal states map to form, and instead think about the end result of the system and how it will be used. Moreover, in some cases, it may be challenging, or even impossible, to map the function of the algorithm to form.</p> </section> </section> <section id="sec-30"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0">9</span></span> CONCLUSIONS</h2> </div> </header> <p>This paper reported a comparative between-groups lab study examining the role of visual feedback in smart camera apps. Leveraging a novel experimental design centered on the creation of stop-motion animations, 40 participants were exposed to four different levels of feedback. Through a combination of quantitative and qualitative methods, our findings indicate a disconnect between user expectations and the information actually represented by the feedback. Participants exposed to keypoint marker feedback derived from early stages of processing showed a tendency to misunderstand it and overall they performed worse than participants who received no feedback at all. Conversely, participants who received keypoint marker feedback derived from later stages of processing demonstrated an improved understanding of the system operation. We conclude that the stage of processing from which feedback is derived plays an important role in users’ ability to develop coherent and correct understandings of a system's operation. We hope that the results presented in this paper will inform the design of feedback in smart camera apps, and other applications of pattern recognition. More generally, we hope that our study method can be used by HCI researchers in future work exploring the design space of feedback and cues.</p> </section> </section> <section class="back-matter"> <section id="sec-31"> <header> <div class="title-info"> <h2> <span class="section-number"><span class="nav-open" id="nav-open" onclick="openNav(this)" title="navigate to" tabindex="0"> </span></span> ACKNOWLEDGMENTS</h2> </div> </header> <p>This work is supported by the Engineering and Physical Sciences Research Council Aperio (EP/L024608/1) and A-IoT (EP/N014243/1) projects. Study approved by the Ethics Committees of UCLIC and of the University of Southampton (ref: 27198). Data URI: <a class="link-inline force-break" href="https://doi.org/10.5258/SOTON/D0757">https://doi.org/10.5258/SOTON/D0757</a>. See supporting materials for image attributions.</p> </section> <section id="ref-001"> <header> <div class="title-info"> <h2 class="page-brake-head"><span class="nav-open" id="nav-open" onclick="openNav(this)" tabindex="0" title="navigate to">REFERENCES</span></h2> </div> </header> <ul class="bibUl"> <li id="BibPLXBIB0001" label="[1]" value="1">Alper&nbsp;T. Alan, Enrico Costanza, Sarvapali&nbsp;D. Ramchurn, Joel Fischer, Tom Rodden, and Nicholas&nbsp;R. Jennings. 2016. Tariff Agent: Interacting with a Future Smart Energy System at Home. <em><em>ACM Trans. Comput.-Hum. Interact.</em></em>23, 4, Article 25 (Aug. 2016), 28&nbsp;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/2943770" target="_blank">https://doi.org/10.1145/2943770</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 1" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000116" aria-label="citation 1 reference 1">citation 1</option></select></li> <li id="BibPLXBIB0002" label="[2]" value="2">Alper&nbsp;T. Alan, Mike Shann, Enrico Costanza, Sarvapali&nbsp;D. Ramchurn, and Sven Seuken. 2016. It is Too Hot: An In-Situ Study of Three Designs for Heating. In <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’16</em>). ACM, New York, NY, USA, 5262–5273. <a class="link-inline force-break" href="https://doi.org/10.1145/2858036.2858222" target="_blank">https://doi.org/10.1145/2858036.2858222</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 2" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000220" aria-label="citation 1 reference 2">citation 1</option></select></li> <li id="BibPLXBIB0003" label="[3]" value="3">Piraye Bayman and Richard&nbsp;E. Mayer. 1984. Instructional manipulation of users’ mental models for electronic calculators. <em><em>International Journal of Man-Machine Studies</em></em>20, 2 (1984), 189 – 199. <a class="link-inline force-break" href="https://doi.org/10.1016/S0020-7373(84)80017-6" target="_blank">https://doi.org/10.1016/S0020-7373(84)80017-6</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 3" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00039" aria-label="citation 1 reference 3">citation 1</option></select></li> <li id="BibPLXBIB0004" label="[4]" value="4">Supriyo Chakraborty, Richard Tomsett, Ramya Raghavendra, Daniel Harborne, Moustafa Alzantot, Federico Cerutti, Mani&nbsp;B. Srivastava, Alun&nbsp;D. Preece, Simon&nbsp;J. Julier, Raghuveer&nbsp;M. Rao, Troy&nbsp;D. Kelley, Dave Braines, Murat Sensoy, Christopher&nbsp;J. Willis, and Prudhvi Gurram. 2017. Interpretability of deep learning models: A survey of results. <em><em>2017 IEEE SmartWorld, Ubiquitous Intelligence &amp; Computing, Advanced &amp; Trusted Computed, Scalable Computing &amp; Communications, Cloud &amp; Big Data Computing, Internet of People and Smart City Innovation</em></em> (2017), 1–6. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 4" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00043" aria-label="citation 1 reference 4">citation 1</option></select></li> <li id="BibPLXBIB0005" label="[5]" value="5">Henriette Cramer, Vanessa Evers, Satyan Ramlal, Maarten van Someren, Lloyd Rutledge, Natalia Stash, Lora Aroyo, and Bob Wielinga. 2008. The effects of transparency on trust in and acceptance of a content-based art recommender. <em><em>User Modeling and User-Adapted Interaction</em></em>18, 5 (20 Aug 2008), 455. <a class="link-inline force-break" href="https://doi.org/10.1007/s11257-008-9051-3" target="_blank">https://doi.org/10.1007/s11257-008-9051-3</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 5" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000512" aria-label="citation 1 reference 5">citation 1</option></select></li> <li id="BibPLXBIB0006" label="[6]" value="6">Graham Dove, Kim Halskov, Jodi Forlizzi, and John Zimmerman. 2017. UX Design Innovation: Challenges for Working with Machine Learning As a Design Material. In <em>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’17</em>). ACM, New York, NY, USA, 278–288. <a class="link-inline force-break" href="https://doi.org/10.1145/3025453.3025739" target="_blank">https://doi.org/10.1145/3025453.3025739</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 6" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00062" aria-label="citation 1 reference 6">citation 1</option><option value="#auto-BibPLXBIB000622" aria-label="citation 2 reference 6">citation 2</option></select></li> <li id="BibPLXBIB0007" label="[7]" value="7">Benedict du Boulay, Tim O'Shea, and John Monk. 1981. The black box inside the glass box: presenting computing concepts to novices. <em><em>International Journal of Man-Machine Studies</em></em>14, 3 (1981), 237 – 249. <a class="link-inline force-break" href="https://doi.org/10.1016/S0020-7373(81)80056-9" target="_blank">https://doi.org/10.1016/S0020-7373(81)80056-9</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 7" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000710" aria-label="citation 1 reference 7">citation 1</option></select></li> <li id="BibPLXBIB0008" label="[8]" value="8">Jerry Fails and Dan Olsen. 2003. A Design Tool for Camera-based Interaction. In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’03</em>). ACM, New York, NY, USA, 449–456. <a class="link-inline force-break" href="https://doi.org/10.1145/642611.642690" target="_blank">https://doi.org/10.1145/642611.642690</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 8" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000825" aria-label="citation 1 reference 8">citation 1</option><option value="#auto-BibPLXBIB000827" aria-label="citation 2 reference 8">citation 2</option></select></li> <li id="BibPLXBIB0009" label="[9]" value="9">Pedro&nbsp;Garcia Garcia, Enrico Costanza, Sarvapali&nbsp;D. Ramchurn, and Jhim Kiel&nbsp;M. Verame. 2016. The Potential of Physical Motion Cues: Changing People's Perception of Robots’ Performance. In <em>Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing</em>(<em>UbiComp ’16</em>). ACM, New York, NY, USA, 510–518. <a class="link-inline force-break" href="https://doi.org/10.1145/2971648.2971697" target="_blank">https://doi.org/10.1145/2971648.2971697</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 9" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB000921" aria-label="citation 1 reference 9">citation 1</option></select></li> <li id="BibPLXBIB0010" label="[10]" value="10">Jun Kato, Sean McDirmid, and Xiang Cao. 2012. DejaVu: Integrated Support for Developing Interactive Camera-based Programs. In <em>Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology</em>(<em>UIST ’12</em>). ACM, New York, NY, USA, 189–196. <a class="link-inline force-break" href="https://doi.org/10.1145/2380116.2380142" target="_blank">https://doi.org/10.1145/2380116.2380142</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 10" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001029" aria-label="citation 1 reference 10">citation 1</option><option value="#auto-BibPLXBIB001034" aria-label="citation 2 reference 10">citation 2</option></select></li> <li id="BibPLXBIB0011" label="[11]" value="11">David&nbsp;E. Kieras and Susan Bovair. 1984. The role of a mental model in learning to operate a device. <em><em>Cognitive Science</em></em>8, 3 (1984), 255 – 273. <a class="link-inline force-break" href="https://doi.org/10.1016/S0364-0213(84)80003-8" target="_blank">https://doi.org/10.1016/S0364-0213(84)80003-8</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 11" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001111" aria-label="citation 1 reference 11">citation 1</option></select></li> <li id="BibPLXBIB0012" label="[12]" value="12">René&nbsp;F. Kizilcec. 2016. How Much Information?: Effects of Transparency on Trust in an Algorithmic Interface. In <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’16</em>). ACM, New York, NY, USA, 2390–2395. <a class="link-inline force-break" href="https://doi.org/10.1145/2858036.2858402" target="_blank">https://doi.org/10.1145/2858036.2858402</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 12" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001219" aria-label="citation 1 reference 12">citation 1</option></select></li> <li id="BibPLXBIB0013" label="[13]" value="13">Jeamin Koo, Jungsuk Kwac, Wendy Ju, Martin Steinert, Larry Leifer, and Clifford Nass. 2015. Why did my car just do that? Explaining semi-autonomous driving actions to improve driver understanding, trust, and performance. <em><em>International Journal on Interactive Design and Manufacturing (IJIDeM)</em></em>9, 4 (01 Nov 2015), 269–275. <a class="link-inline force-break" href="https://doi.org/10.1007/s12008-014-0227-2" target="_blank">https://doi.org/10.1007/s12008-014-0227-2</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 13" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001324" aria-label="citation 1 reference 13">citation 1</option></select></li> <li id="BibPLXBIB0014" label="[14]" value="14">Josua Krause, Adam Perer, and Kenney Ng. 2016. Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models. In <em>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’16</em>). ACM, New York, NY, USA, 5686–5697. <a class="link-inline force-break" href="https://doi.org/10.1145/2858036.2858529" target="_blank">https://doi.org/10.1145/2858036.2858529</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 14" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001432" aria-label="citation 1 reference 14">citation 1</option></select></li> <li id="BibPLXBIB0015" label="[15]" value="15">Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf. 2015. Principles of Explanatory Debugging to Personalize Interactive Machine Learning. In <em>Proceedings of the 20th International Conference on Intelligent User Interfaces</em>(<em>IUI ’15</em>). ACM, New York, NY, USA, 126–137. <a class="link-inline force-break" href="https://doi.org/10.1145/2678025.2701399" target="_blank">https://doi.org/10.1145/2678025.2701399</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 15" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001514" aria-label="citation 1 reference 15">citation 1</option></select></li> <li id="BibPLXBIB0016" label="[16]" value="16">Todd Kulesza, Simone Stumpf, Margaret Burnett, and Irwin Kwan. 2012. Tell Me More?: The Effects of Mental Model Soundness on Personalizing an Intelligent Agent. In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’12</em>). ACM, New York, NY, USA, 1–10. <a class="link-inline force-break" href="https://doi.org/10.1145/2207676.2207678" target="_blank">https://doi.org/10.1145/2207676.2207678</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 16" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001613" aria-label="citation 1 reference 16">citation 1</option></select></li> <li id="BibPLXBIB0017" label="[17]" value="17">Brian&nbsp;Y. Lim, Anind&nbsp;K. Dey, and Daniel Avrahami. 2009. Why and Why Not Explanations Improve the Intelligibility of Context-aware Intelligent Systems. In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’09</em>). ACM, New York, NY, USA, 2119–2128. <a class="link-inline force-break" href="https://doi.org/10.1145/1518701.1519023" target="_blank">https://doi.org/10.1145/1518701.1519023</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 17" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001723" aria-label="citation 1 reference 17">citation 1</option></select></li> <li id="BibPLXBIB0018" label="[18]" value="18">Dan Maynes-Aminzade, Terry Winograd, and Takeo Igarashi. 2007. Eyepatch: Prototyping Camera-based Interaction Through Examples. In <em>Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology</em>(<em>UIST ’07</em>). ACM, New York, NY, USA, 33–42. <a class="link-inline force-break" href="https://doi.org/10.1145/1294211.1294219" target="_blank">https://doi.org/10.1145/1294211.1294219</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 18" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB001826" aria-label="citation 1 reference 18">citation 1</option></select></li> <li id="BibPLXBIB0019" label="[19]" value="19">Neville Moray. 1999. Mental models in theory and practice. <em><em>Attention and performance XVII: Cognitive regulation of performance: Interaction of theory and application</em></em>(1999), 223–258. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 19" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00197" aria-label="citation 1 reference 19">citation 1</option></select></li> <li id="BibPLXBIB0020" label="[20]" value="20">Donald Norman. 2014. <em>On the relationship between conceptual and mental models. In Gentner et al (e.d) Mental Models</em>. Psychology Press. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 20" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00208" aria-label="citation 1 reference 20">citation 1</option></select></li> <li id="BibPLXBIB0021" label="[21]" value="21">Kayur Patel, Naomi Bancroft, Steven&nbsp;M. Drucker, James Fogarty, Andrew&nbsp;J. Ko, and James Landay. 2010. Gestalt: Integrated Support for Implementation and Analysis in Machine Learning. In <em>Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology</em>(<em>UIST ’10</em>). ACM, New York, NY, USA, 37–46. <a class="link-inline force-break" href="https://doi.org/10.1145/1866029.1866038" target="_blank">https://doi.org/10.1145/1866029.1866038</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 21" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002131" aria-label="citation 1 reference 21">citation 1</option></select></li> <li id="BibPLXBIB0022" label="[22]" value="22">Kayur Patel, James Fogarty, James&nbsp;A. Landay, and Beverly Harrison. 2008. Investigating Statistical Machine Learning As a Tool for Software Development. In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’08</em>). ACM, New York, NY, USA, 667–676. <a class="link-inline force-break" href="https://doi.org/10.1145/1357054.1357160" target="_blank">https://doi.org/10.1145/1357054.1357160</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 22" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002228" aria-label="citation 1 reference 22">citation 1</option></select></li> <li id="BibPLXBIB0023" label="[23]" value="23">Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. 2011. ORB: An Efficient Alternative to SIFT or SURF. In <em>Proceedings of the 2011 International Conference on Computer Vision</em>(<em>ICCV ’11</em>). IEEE Computer Society, Washington, DC, USA, 2564–2571. <a class="link-inline force-break" href="https://doi.org/10.1109/ICCV.2011.6126544" target="_blank">https://doi.org/10.1109/ICCV.2011.6126544</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 23" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002336" aria-label="citation 1 reference 23">citation 1</option></select></li> <li id="BibPLXBIB0024" label="[24]" value="24">K. Simonyan, A. Vedaldi, and A. Zisserman. 2014. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. In <em>Workshop at International Conference on Learning Representations</em>. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 24" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00244" aria-label="citation 1 reference 24">citation 1</option></select></li> <li id="BibPLXBIB0025" label="[25]" value="25">Louis&nbsp;H Sullivan. 1896. The tall office building artistically considered. <em><em>Lippincott's Magazine</em></em>57, 3 (1896), 406. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 25" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002535" aria-label="citation 1 reference 25">citation 1</option></select></li> <li id="BibPLXBIB0026" label="[26]" value="26">Joe Tullio, Anind&nbsp;K. Dey, Jason Chalecki, and James Fogarty. 2007. How It Works: A Field Study of Non-technical Users Interacting with an Intelligent System. In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’07</em>). ACM, New York, NY, USA, 31–40. <a class="link-inline force-break" href="https://doi.org/10.1145/1240624.1240630" target="_blank">https://doi.org/10.1145/1240624.1240630</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 26" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002615" aria-label="citation 1 reference 26">citation 1</option><option value="#auto-BibPLXBIB002633" aria-label="citation 2 reference 26">citation 2</option></select></li> <li id="BibPLXBIB0027" label="[27]" value="27">Qian Yang, Nikola Banovic, and John Zimmerman. 2018. Mapping Machine Learning Advances from HCI Research to Reveal Starting Places for Design Innovation. In <em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’18</em>). ACM, New York, NY, USA, Article 130, 11&nbsp;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/3173574.3173704" target="_blank">https://doi.org/10.1145/3173574.3173704</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 27" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00271" aria-label="citation 1 reference 27">citation 1</option></select></li> <li id="BibPLXBIB0028" label="[28]" value="28">Rayoung Yang and Mark&nbsp;W. Newman. 2013. Learning from a Learning Thermostat: Lessons for Intelligent Systems for the Home. In <em>Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing</em>(<em>UbiComp ’13</em>). ACM, New York, NY, USA, 93–102. <a class="link-inline force-break" href="https://doi.org/10.1145/2493432.2493489" target="_blank">https://doi.org/10.1145/2493432.2493489</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 28" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00286" aria-label="citation 1 reference 28">citation 1</option><option value="#auto-BibPLXBIB002818" aria-label="citation 2 reference 28">citation 2</option></select></li> <li id="BibPLXBIB0029" label="[29]" value="29">Rayoung Yang, Mark&nbsp;W. Newman, and Jodi Forlizzi. 2014. Making Sustainability Sustainable: Challenges in the Design of Eco-interaction Technologies. In <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>(<em>CHI ’14</em>). ACM, New York, NY, USA, 823–832. <a class="link-inline force-break" href="https://doi.org/10.1145/2556288.2557380" target="_blank">https://doi.org/10.1145/2556288.2557380</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 29" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB002917" aria-label="citation 1 reference 29">citation 1</option></select></li> <li id="BibPLXBIB0030" label="[30]" value="30">Matthew&nbsp;D. Zeiler and Rob Fergus. 2014. Visualizing and Understanding Convolutional Networks. In <em>Computer Vision</em>, David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (Eds.). Springer International Publishing, Cham, 818–833. <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 30" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB00305" aria-label="citation 1 reference 30">citation 1</option></select></li> <li id="BibPLXBIB0031" label="[31]" value="31">Yuhang Zhao, Sarit Szpiro, Jonathan Knighten, and Shiri Azenkot. 2016. CueSee: Exploring Visual Cues for People with Low Vision to Facilitate a Visual Search Task. In <em>Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing</em>(<em>UbiComp ’16</em>). ACM, New York, NY, USA, 73–84. <a class="link-inline force-break" href="https://doi.org/10.1145/2971648.2971730" target="_blank">https://doi.org/10.1145/2971648.2971730</a> <span class="link-das"> </span><select class="bib-ref-num" aria-label="Jump to citation for reference 31" style="display:inline-block;"><option>Navigate to</option><option value="#auto-BibPLXBIB003130" aria-label="citation 1 reference 31">citation 1</option></select></li> </ul> </section> </section> <section id="foot-001" class="footnote"> <header> <div class="title-info"> <h2><span class="nav-open" id="nav-open" onclick="openNav(this)" tabindex="0" title="navigate to">FOOTNOTE</span></h2> </div> </header> <p id="fn1"><a href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#foot-fn1"><sup>1</sup></a>which tries to find matching images from an internet search</p> <p id="fn2"><a href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#foot-fn2"><sup>2</sup></a>e.g. OpenCV <a class="link-inline force-break" href="https://goo.gl/bX4XEM">https://goo.gl/bX4XEM</a> </p> <p id="fn3"><a href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#foot-fn3"><sup>3</sup></a>For more information about the Anim8 app visit: <a class="link-inline force-break" href="http://anim8.space/">http://anim8.space/</a> </p> <p id="fn4"><a href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#foot-fn4"><sup>4</sup></a>Through experimentation the ORB algorithm [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#BibPLXBIB0023" data-original-title="" title="" id="auto-BibPLXBIB002336">23</a>] proved to offer the best compromise of performance, speed and control for our study.</p> <p id="fn5"><a href="http://delivery.acm.org/10.1145/3310000/3300273/a43-kittley-davies.html?ip=169.237.6.227&amp;id=3300273&amp;acc=ACTIVE%20SERVICE&amp;key=CA367851C7E3CE77%2EBD0EBCE24FE9A3C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;__acm__=1575108218_74179ec5c6660eb7e468bcdb7e3c2dec#foot-fn5"><sup>5</sup></a>understanding on Task 3: chi-square=3.509, p=.320, df=3, Cramer's V=0.296; understanding on Task 4: chi-square=5.812, p=.121, df=3, Cramer's V=0.381; correct selections on Task 3: chi-square=6.667,p=.083,df=3; all selections were correct in Task 4, so no statistical test needed</p> <div class="bibStrip"> <p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from <a href="mailto:permissions@acm.org">permissions@acm.org</a>.</p> <p><em>CHI '19, May 04–09, 2019, Glasgow, Scotland UK</em></p> <p>© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM<br> ACM ISBN 978-1-4503-5970-2/19/05…$15.00.<br>DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3290605.3300273">https://doi.org/10.1145/3290605.3300273</a> </p> </div> </section>   
</body></html>